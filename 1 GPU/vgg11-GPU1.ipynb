{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM9Pz/LTkaN3skdRsM+jIC3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmCxNTQ4OZL2","executionInfo":{"status":"ok","timestamp":1736360210175,"user_tz":-60,"elapsed":275,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"0534ab88-b505-476c-f8eb-4585357c8fdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}],"source":["%%writefile main.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import _LRScheduler\n","import torch.utils.data as data\n","\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","\n","from sklearn import decomposition\n","from sklearn import manifold\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from tqdm.notebook import tqdm, trange\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import copy\n","import random\n","import time\n","import os\n","import json\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","SEED = 1234\n","ROOT = \".\"\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","\"\"\"# 2. Initialize the DDP Environment\"\"\"\n","\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n","    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()\n","\n","\"\"\"# 3. Define a Model.\"\"\"\n","\n","class VGG(nn.Module):\n","    def __init__(self, features, output_dim):\n","        super().__init__()\n","\n","        self.features = features\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d(7)\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        h = x.view(x.shape[0], -1)\n","        x = self.classifier(h)\n","        return x, h\n","\n","def get_vgg_layers(config, batch_norm):\n","\n","    layers = []\n","    in_channels = 3\n","\n","    for c in config:\n","        assert c == 'M' or isinstance(c, int)\n","        if c == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = c\n","\n","    return nn.Sequential(*layers)\n","\n","vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n","OUTPUT_DIM = 10\n","vgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n","\n","# model = VGG(vgg11_layers, OUTPUT_DIM)\n","\n","pretrained_model = models.vgg11_bn(pretrained=True)\n","IN_FEATURES = pretrained_model.classifier[-1].in_features\n","\n","final_fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n","\n","pretrained_model.classifier[-1] = final_fc\n","\n","\n","# model.load_state_dict(pretrained_model.state_dict())\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def create_model():\n","    model = VGG(vgg11_layers, OUTPUT_DIM)\n","    print(f'Befor use pretraind: The model has {count_parameters(model):,} trainable parameters')\n","\n","    model.load_state_dict(pretrained_model.state_dict())\n","    for parameter in model.features.parameters():\n","        parameter.requires_grad = False\n","\n","    for parameter in model.classifier[:-1].parameters():\n","        parameter.requires_grad = False\n","\n","    print(f'After use pretraind: The model has {count_parameters(model):,} trainable parameters')\n","\n","    return model\n","\n","\"\"\"# 4. Create a Dummy Dataset\"\"\"\n","\n","def create_dataloader(rank, world_size, batch_size=32, root = ROOT):\n","    ## defined transforms\n","    pretrained_size = 224\n","    pretrained_means = [0.485, 0.456, 0.406]\n","    pretrained_stds = [0.229, 0.224, 0.225]\n","\n","    train_transforms = transforms.Compose([\n","                              transforms.Resize(pretrained_size),\n","                              transforms.RandomRotation(5),\n","                              transforms.RandomHorizontalFlip(0.5),\n","                              transforms.RandomCrop(pretrained_size, padding=10),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize(mean=pretrained_means,\n","                                                    std=pretrained_stds)\n","                          ])\n","\n","    test_transforms = transforms.Compose([\n","                              transforms.Resize(pretrained_size),\n","                              transforms.ToTensor(),\n","                              transforms.Normalize(mean=pretrained_means,\n","                                                    std=pretrained_stds)\n","                          ])\n","\n","    ## load the data with\n","    outdir = f'{root}/data'\n","    if rank == 0 and not os.path.exists(outdir):\n","        train_data = datasets.CIFAR10(outdir,\n","                                  train=True,\n","                                  download=True,\n","                                  transform=train_transforms)\n","\n","        test_data = datasets.CIFAR10(outdir,\n","                                train=False,\n","                                download=True,\n","                                transform=test_transforms)\n","\n","    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n","    train_data = datasets.CIFAR10(outdir,\n","                                  train=True,\n","                                  download=True,\n","                                  transform=train_transforms)\n","\n","    test_data = datasets.CIFAR10(outdir,\n","                                train=False,\n","                                download=True,\n","                                transform=test_transforms)\n","\n","    ## create the validation split\n","    VALID_RATIO = 0.9\n","\n","    n_train_examples = int(len(train_data) * VALID_RATIO)\n","    n_valid_examples = len(train_data) - n_train_examples\n","\n","    train_data, valid_data = data.random_split(train_data,\n","                                              [n_train_examples, n_valid_examples])\n","\n","\n","    ## ensure the validation data uses the test transforms\n","    valid_data = copy.deepcopy(valid_data)\n","    valid_data.dataset.transform = test_transforms\n","\n","    ## print\n","    print(f'Number of training examples: {len(train_data)}')\n","    print(f'Number of validation examples: {len(valid_data)}')\n","    print(f'Number of testing examples: {len(test_data)}')\n","\n","\n","\n","    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n","    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n","\n","    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n","\n","\n","    return train_dataloader, val_dataloader, test_dataloader\n","\n","\"\"\"# 5. Implement the Training Loop\n","\n","## a. Help function\n","\"\"\"\n","\n","RESULTS_FILE = f\"{ROOT}/distributed_training_results.json\"\n","\n","def log_results(scenario, results):\n","    \"\"\"\n","    Save results to a JSON file for comparison across scenarios.\n","    \"\"\"\n","    if os.path.exists(RESULTS_FILE):\n","        with open(RESULTS_FILE, 'r') as f:\n","            all_results = json.load(f)\n","    else:\n","        all_results = {}\n","\n","    all_results[scenario] = results\n","\n","    with open(RESULTS_FILE, 'w') as f:\n","        json.dump(all_results, f, indent=4)\n","\n","def calculate_accuracy(y_pred, y):\n","    top_pred = y_pred.argmax(1, keepdim=True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\"\"\"## b. train function\"\"\"\n","\n","def train(model, iterator, optimizer, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    i = 0\n","    for (x, y) in tqdm(iterator, desc= f\"Training on the rank {rank}\", leave=False):\n","\n","        x = x.to(rank)\n","        y = y.to(rank)\n","\n","        optimizer.zero_grad()\n","\n","        y_pred, _ = model(x)\n","\n","        loss = criterion(y_pred, y)\n","\n","        acc = calculate_accuracy(y_pred, y)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        i+=1\n","        if i % 50 == 0 and rank == 0:\n","            print(f\"- {i} was passed over {len(iterator)}\")\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\"\"\"## c. Validation function\"\"\"\n","\n","def evaluate(model, iterator, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","    i=0\n","    with torch.no_grad():\n","\n","        for (x, y) in tqdm(iterator, desc=f\"Evaluating on the rank {rank}\", leave=False):\n","\n","            x = x.to(rank)\n","            y = y.to(rank)\n","\n","            y_pred, _ = model(x)\n","\n","            loss = criterion(y_pred, y)\n","\n","            acc = calculate_accuracy(y_pred, y)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","            i+=1\n","            if i % 50 == 0 and rank == 0:\n","                print(f\"- {i} was passed over {len(iterator)}\")\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","outdir = f'{ROOT}/model/'\n","if not os.path.exists(outdir):\n","    os.makedirs(outdir)\n","\n","def main_train(rank, world_size, root = outdir, num_epochs = 3):\n","    ## a. Set up the distributed process groups\n","    setup(rank, world_size)\n","    print(f\"Process {rank} initialized.\")\n","\n","    ## b. Create Model, DataLoader\n","    model = create_model().to(rank)  # Move model to GPU\n","    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n","\n","    ## c. Wrap the model with DistributedDataParallel\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    ## d. Loss and Optimizer\n","    LR = 5e-4\n","    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n","    optimizer = optim.Adam(ddp_model.parameters(), lr=LR)\n","\n","    ## e. Training Loop\n","    best_valid_loss = float('inf')\n","    training_times = []\n","    train_losses = []\n","    train_accurcy = []\n","    validation_times = []\n","    validation_losses = []\n","    validation_accurcy = []\n","\n","    epoch_times = []\n","\n","    for epoch in trange(num_epochs, desc=\"Epochs\"):\n","        start_epoch_time = time.monotonic()\n","        start_time = time.monotonic()\n","\n","        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n","        train_time = time.monotonic() - start_time\n","        training_times.append(train_time)\n","        train_losses.append(train_loss)\n","        train_accurcy.append(train_acc)\n","\n","        start_time = time.monotonic()\n","        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n","        val_time = time.monotonic() - start_time\n","        validation_times.append(val_time)\n","        validation_losses.append(valid_loss)\n","        validation_accurcy.append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(ddp_model.state_dict(), f'{root}tut4-model.pt')\n","\n","        end_time = time.monotonic()\n","        e_time = end_time - start_epoch_time\n","        epoch_times.append(e_time)\n","        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n","\n","        print(f'--------------|     On process {rank}      |----------------')\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ## f. test after train\n","    ddp_model.load_state_dict(torch.load(f'{root}tut4-model.pt'))\n","    start_time = time.monotonic()\n","    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank)\n","    test_time = time.monotonic() - start_time\n","    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # Log results\n","    results = {\n","        \"world_size\": world_size,\n","        \"rank\": rank,\n","        \"training_times\": training_times,\n","        \"train_losses\": train_losses,\n","        \"train_accurcy\": train_accurcy,\n","        \"validation_times\": validation_times,\n","        \"validation_losses\": validation_losses,\n","        \"validation_accurcy\": validation_accurcy,\n","        \"test_time\": test_time,\n","        \"test_loss\": test_loss,\n","        \"test_acc\": test_acc,\n","        \"epoch_times\": epoch_times\n","     }\n","\n","    model_name = \"vgg11\"\n","    scenario = f\"{model_name}_{world_size}_GPUs\"\n","    log_results(scenario, results)\n","    dist.barrier()\n","\n","    cleanup()\n","    print(f'Process {rank} finished training.')\n","\n","\"\"\"# 6. Main Execution\"\"\"\n","if __name__ == \"__main__\":\n","\n","    def main():\n","        world_size = torch.cuda.device_count()\n","        print(f'Total number of devices detected: {world_size}')\n","\n","        if world_size >= 1:\n","            #start the training process on all available GPUs\n","            if world_size > 1:\n","                #start the training process on all available GPUs\n","                mp.spawn(\n","                    main_train,\n","                    args=(world_size,),\n","                    nprocs=world_size,\n","                    join=True\n","                )\n","            else:\n","                #run training on single GPU\n","                main_train(rank=0, world_size=1)\n","\n","        else:\n","            print('no GPUs found. Please make sure you have configured CUDA correctly')\n","\n","    main()"]},{"cell_type":"code","source":["!python main.py"],"metadata":{"id":"6qIeWD3YPUXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736360988910,"user_tz":-60,"elapsed":777596,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"610641ea-7312-4bd8-d18c-2f4ea3ad824a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Total number of devices detected: 1\n","Process 0 initialized.\n","Befor use pretraind: The model has 128,812,810 trainable parameters\n","After use pretraind: The model has 40,970 trainable parameters\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","100% 170M/170M [00:05<00:00, 28.9MB/s]\n","Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","[rank0]:[W108 18:17:07.655841876 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Number of training examples: 45000\n","Number of validation examples: 5000\n","Number of testing examples: 10000\n","Epochs:   0%|          | 0/3 [00:00<?, ?it/s]\n","Training on the rank 0:   0%|          | 0/1407 [00:00<?, ?it/s]\n","- 50 was passed over 1407\n","- 100 was passed over 1407\n","- 150 was passed over 1407\n","- 200 was passed over 1407\n","- 250 was passed over 1407\n","- 300 was passed over 1407\n","- 350 was passed over 1407\n","- 400 was passed over 1407\n","- 450 was passed over 1407\n","- 500 was passed over 1407\n","- 550 was passed over 1407\n","- 600 was passed over 1407\n","- 650 was passed over 1407\n","- 700 was passed over 1407\n","- 750 was passed over 1407\n","- 800 was passed over 1407\n","- 850 was passed over 1407\n","- 900 was passed over 1407\n","- 950 was passed over 1407\n","- 1000 was passed over 1407\n","- 1050 was passed over 1407\n","- 1100 was passed over 1407\n","- 1150 was passed over 1407\n","- 1200 was passed over 1407\n","- 1250 was passed over 1407\n","- 1300 was passed over 1407\n","- 1350 was passed over 1407\n","- 1400 was passed over 1407\n","Evaluating on the rank 0:   0%|          | 0/157 [00:00<?, ?it/s]\n","- 50 was passed over 157\n","- 100 was passed over 157\n","- 150 was passed over 157\n","--------------|     On process 0      |----------------\n","Epoch: 01 | Epoch Time: 3m 57s\n","\tTrain Loss: 0.890 | Train Acc: 69.36%\n","\t Val. Loss: 0.702 |  Val. Acc: 75.66%\n","Training on the rank 0:   0%|          | 0/1407 [00:00<?, ?it/s]\n","- 50 was passed over 1407\n","- 100 was passed over 1407\n","- 150 was passed over 1407\n","- 200 was passed over 1407\n","- 250 was passed over 1407\n","- 300 was passed over 1407\n","- 350 was passed over 1407\n","- 400 was passed over 1407\n","- 450 was passed over 1407\n","- 500 was passed over 1407\n","- 550 was passed over 1407\n","- 600 was passed over 1407\n","- 650 was passed over 1407\n","- 700 was passed over 1407\n","- 750 was passed over 1407\n","- 800 was passed over 1407\n","- 850 was passed over 1407\n","- 900 was passed over 1407\n","- 950 was passed over 1407\n","- 1000 was passed over 1407\n","- 1050 was passed over 1407\n","- 1100 was passed over 1407\n","- 1150 was passed over 1407\n","- 1200 was passed over 1407\n","- 1250 was passed over 1407\n","- 1300 was passed over 1407\n","- 1350 was passed over 1407\n","- 1400 was passed over 1407\n","Evaluating on the rank 0:   0%|          | 0/157 [00:00<?, ?it/s]\n","- 50 was passed over 157\n","- 100 was passed over 157\n","- 150 was passed over 157\n","--------------|     On process 0      |----------------\n","Epoch: 02 | Epoch Time: 3m 57s\n","\tTrain Loss: 0.793 | Train Acc: 72.22%\n","\t Val. Loss: 0.653 |  Val. Acc: 76.89%\n","Training on the rank 0:   0%|          | 0/1407 [00:00<?, ?it/s]\n","- 50 was passed over 1407\n","- 100 was passed over 1407\n","- 150 was passed over 1407\n","- 200 was passed over 1407\n","- 250 was passed over 1407\n","- 300 was passed over 1407\n","- 350 was passed over 1407\n","- 400 was passed over 1407\n","- 450 was passed over 1407\n","- 500 was passed over 1407\n","- 550 was passed over 1407\n","- 600 was passed over 1407\n","- 650 was passed over 1407\n","- 700 was passed over 1407\n","- 750 was passed over 1407\n","- 800 was passed over 1407\n","- 850 was passed over 1407\n","- 900 was passed over 1407\n","- 950 was passed over 1407\n","- 1000 was passed over 1407\n","- 1050 was passed over 1407\n","- 1100 was passed over 1407\n","- 1150 was passed over 1407\n","- 1200 was passed over 1407\n","- 1250 was passed over 1407\n","- 1300 was passed over 1407\n","- 1350 was passed over 1407\n","- 1400 was passed over 1407\n","Evaluating on the rank 0:   0%|          | 0/157 [00:00<?, ?it/s]\n","- 50 was passed over 157\n","- 100 was passed over 157\n","- 150 was passed over 157\n","--------------|     On process 0      |----------------\n","Epoch: 03 | Epoch Time: 3m 59s\n","\tTrain Loss: 0.786 | Train Acc: 72.55%\n","\t Val. Loss: 0.641 |  Val. Acc: 77.43%\n","/content/main.py:373: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ddp_model.load_state_dict(torch.load(f'{root}tut4-model.pt'))\n","Evaluating on the rank 0:   0%|          | 0/313 [00:00<?, ?it/s]\n","- 50 was passed over 313\n","- 100 was passed over 313\n","- 150 was passed over 313\n","- 200 was passed over 313\n","- 250 was passed over 313\n","- 300 was passed over 313\n","Test results on process 0: Test Loss: 0.653 | Test Acc: 76.76%\n","Process 0 finished training.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DPWdFt8yIRmN"},"execution_count":null,"outputs":[]}]}