{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXob/oIvS0vr/5uUNJ7D+o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gm_jlqCRHu1w"},"outputs":[],"source":["%%writefile main.py\n","\n","import torch\n","import numpy as np\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils import data\n","from tqdm import tqdm, trange\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","import copy\n","import random\n","import time\n","import os\n","import json\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","SEED = 1234\n","ROOT = \".\"\n","MODEL_NAME = \"CNN\"\n","SENARIO = \"1GPU\"\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","\"\"\"# 2. Initialize the DDP Environment\"\"\"\n","\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n","    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()\n","\n","\"\"\"# 3. Define a Model.\"\"\"\n","\n","\n","# define the CNN architecture\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # convolutional layer (sees 32x32x3 image tensor)\n","        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n","        # convolutional layer (sees 16x16x16 tensor)\n","        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n","        # convolutional layer (sees 8x8x32 tensor)\n","        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n","        # max pooling layer\n","        self.pool = nn.MaxPool2d(2, 2)\n","        # linear layer (64 * 4 * 4 -> 500)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n","        # linear layer (500 -> 10)\n","        self.fc2 = nn.Linear(500, 10)\n","        # dropout layer (p=0.25)\n","        self.dropout = nn.Dropout(0.25)\n","\n","    def forward(self, x):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        # flatten image input\n","        x = x.view(-1, 64 * 4 * 4)\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add 1st hidden layer, with relu activation function\n","        x = F.relu(self.fc1(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add 2nd hidden layer, with relu activation function\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def create_model():\n","\n","    model = CNN()\n","    print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","    return model\n","\n","\"\"\"# 4. Create a Dummy Dataset\"\"\"\n","\n","def create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n","    train_transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(), # randomly flip and rotate\n","        transforms.RandomRotation(10),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","\n","    ## load the data with\n","    outdir = f\"{root}/data\"\n","    if rank == 0 and not os.path.exists(outdir):\n","        train_data = datasets.CIFAR10(outdir, train=True,\n","                                      download=True, transform=train_transform)\n","        test_data = datasets.CIFAR10(outdir, train=False,\n","                                    download=True, transform=test_transform)\n","\n","    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n","\n","    train_data = datasets.CIFAR10(outdir, train=True,\n","                                  download=True, transform=train_transform)\n","    test_data = datasets.CIFAR10(outdir, train=False,\n","                                download=True, transform=test_transform)\n","\n","    ## create the validation split\n","    VALID_RATIO = 0.9\n","\n","    n_train_examples = int(len(train_data) * VALID_RATIO)\n","    n_valid_examples = len(train_data) - n_train_examples\n","    train_data, valid_data = data.random_split(train_data,\n","                                           [n_train_examples, n_valid_examples])\n","\n","    if rank == 0:\n","        print(f'Number of training examples: {len(train_data)}')\n","        print(f'Number of validation examples: {len(valid_data)}')\n","        print(f'Number of testing examples: {len(test_data)}')\n","\n","\n","    ## Creating Data Loaders\n","\n","    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n","    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n","\n","    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n","    return train_dataloader, val_dataloader, test_dataloader\n","\n","\"\"\"# 5. Implement the Training Loop\n","\n","## a. Help function\n","\"\"\"\n","\n","RESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n","\n","def log_results(scenario, results):\n","    \"\"\"\n","    Save results to a JSON file for comparison across scenarios.\n","    \"\"\"\n","    if os.path.exists(RESULTS_FILE):\n","        with open(RESULTS_FILE, 'r') as f:\n","            all_results = json.load(f)\n","    else:\n","        all_results = {}\n","\n","    all_results[scenario] = results\n","\n","    with open(RESULTS_FILE, 'w') as f:\n","        json.dump(all_results, f, indent=4)\n","\n","def calculate_accuracy(y_pred, y):\n","    top_pred = y_pred.argmax(1, keepdim=True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\"\"\"## b. train function\"\"\"\n","def train(model, iterator, optimizer, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    i=0\n","    for (x, y) in tqdm(iterator, desc=f\"Training on the rank {rank}...\", leave=False):\n","\n","        x = x.to(rank)\n","        y = y.to(rank)\n","\n","        optimizer.zero_grad()\n","\n","        y_pred, _ = model(x)\n","\n","        loss = criterion(y_pred, y)\n","\n","        acc = calculate_accuracy(y_pred, y)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        if i % 50 == 0 and rank == 0 :\n","            print(f\"- On Training: {i} was passed over  {len(iterator)}\")\n","        i+=1\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\n","\"\"\"## c. Validation function\"\"\"\n","def evaluate(model, iterator, criterion, rank, mode = \"Evaluating\"):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","    i=0\n","    with torch.no_grad():\n","\n","        for (x, y) in tqdm(iterator, desc=f\"{mode} on the rank {rank} ...\", leave=False):\n","\n","            x = x.to(rank)\n","            y = y.to(rank)\n","\n","            y_pred, _ = model(x)\n","\n","            loss = criterion(y_pred, y)\n","\n","            acc = calculate_accuracy(y_pred, y)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","            if i % 50 == 0 and rank == 0:\n","                print(f\"- On {mode}: {i} was passed over  {len(iterator)}\")\n","            i+=1\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\n","\"\"\"## d. Main loop\"\"\"\n","\n","outdir = f'{ROOT}/model/'\n","if not os.path.exists(outdir):\n","    os.makedirs(outdir)\n","\n","def main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n","    ## a. Set up the distributed process groups\n","    setup(rank, world_size)\n","    print(f\"Process {rank} initialized.\")\n","\n","    # setup mp_model and devices for this process\n","\n","\n","    ## b. Create Model, DataLoader\n","    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n","    model = create_model().to(rank)\n","\n","    ## c. Wrap the model with DistributedDataParallel\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    ## d. Loss and Optimizer\n","    #LR = 5e-4\n","    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n","    optimizer = optim.Adam(ddp_model.parameters(), lr=0.01)\n","\n","    ## e. Training Loop\n","    best_valid_loss = float('inf')\n","    training_times = []\n","    train_losses = []\n","    train_accurcy = []\n","    validation_times = []\n","    validation_losses = []\n","    validation_accurcy = []\n","\n","    epoch_times = []\n","\n","    for epoch in trange(num_epochs, desc=\"Epochs\"):\n","        start_epoch_time = time.monotonic()\n","        start_time = time.monotonic()\n","\n","        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n","        train_time = time.monotonic() - start_time\n","        training_times.append(train_time)\n","        train_losses.append(train_loss)\n","        train_accurcy.append(train_acc)\n","\n","        start_time = time.monotonic()\n","        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n","        val_time = time.monotonic() - start_time\n","        validation_times.append(val_time)\n","        validation_losses.append(valid_loss)\n","        validation_accurcy.append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(ddp_model.state_dict(), f'{root}mlp-model.pt')\n","\n","        end_time = time.monotonic()\n","        e_time = end_time - start_epoch_time\n","        epoch_times.append(e_time)\n","        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n","\n","        print(f'--------------|     On process {rank}      |----------------')\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ## f. test after train\n","    ddp_model.load_state_dict(torch.load(f'{root}mlp-model.pt'))\n","    start_time = time.monotonic()\n","    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n","    test_time = time.monotonic() - start_time\n","    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # Log results\n","    results = {\n","        \"world_size\": world_size,\n","        \"rank\": rank,\n","        \"training_times\": training_times,\n","        \"train_losses\": train_losses,\n","        \"train_accurcy\": train_accurcy,\n","        \"validation_times\": validation_times,\n","        \"validation_losses\": validation_losses,\n","        \"validation_accurcy\": validation_accurcy,\n","        \"test_time\": test_time,\n","        \"test_loss\": test_loss,\n","        \"test_acc\": test_acc,\n","        \"epoch_times\": epoch_times\n","     }\n","\n","    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n","    log_results(scenario, results)\n","    dist.barrier()\n","\n","    cleanup()\n","    print(f'Process {rank} finished training.')\n","\n","\"\"\"# 6. Main Execution\"\"\"\n","if __name__ == \"__main__\":\n","\n","    def main():\n","        world_size = torch.cuda.device_count()\n","        print(f'Total number of devices detected: {world_size}')\n","\n","        if world_size >= 1:\n","            #start the training process on all available GPUs\n","\n","            if world_size > 1:\n","                #start the training process on all available GPUs\n","\n","                mp.spawn(\n","                    main_train,\n","                    args=(world_size,),\n","                    nprocs=world_size,\n","                    join=True\n","                )\n","            else:\n","                #run training on single GPU\n","                main_train(rank=0, world_size=1)\n","\n","        else:\n","            print('no GPUs found. Please make sure you have configured CUDA correctly')\n","\n","    main()"]},{"cell_type":"code","source":["!python  main.py"],"metadata":{"id":"Em_5zrc3LLDB"},"execution_count":null,"outputs":[]}]}