{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMqqoFZD9viX3OnqQRIHJ8/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"ftk226Wxeiwz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736438273533,"user_tz":-60,"elapsed":421,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"bffc1825-aece-4e09-dd8e-5c9a23584262"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}],"source":["%%writefile main.py\n","\n","import torch\n","import numpy as np\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm.notebook import tqdm, trange\n","\n","\n","import copy\n","import random\n","import time\n","import os\n","import json\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","SEED = 1234\n","ROOT = \".\"\n","MODEL_NAME = \"ConvAutoencoder\"\n","SENARIO = \"1GPU\"\n","EPOCHS = 10\n","BATCH_SIZE = 64\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","\"\"\"# 2. Initialize the DDP Environment\"\"\"\n","\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n","    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()\n","\n","\"\"\"# 3. Define a Model.\"\"\"\n","\n","class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        ## encoder layers ##\n","        # conv layer (depth from 1 --> 16), 3x3 kernels\n","        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n","        # conv layer (depth from 16 --> 4), 3x3 kernels\n","        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n","        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        ## decoder layers ##\n","        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n","        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n","        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n","\n","\n","    def forward(self, x):\n","        ## encode ##\n","        # add hidden layers with relu activation function\n","        # and maxpooling after\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        # add second hidden layer\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)  # compressed representation\n","\n","        ## decode ##\n","        # add transpose conv layers, with relu activation function\n","        x = F.relu(self.t_conv1(x))\n","        # output layer (with sigmoid for scaling from 0 to 1)\n","        x = F.sigmoid(self.t_conv2(x))\n","\n","        return x\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def create_model():\n","    model = ConvAutoencoder()\n","\n","    print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","    return model\n","\n","\"\"\"# 4. Create a Dummy Dataset\"\"\"\n","\n","def create_dataloader(rank, world_size, batch_size=32, root = ROOT):\n","    ## defined transforms\n","    transform = transforms.ToTensor()\n","\n","    ## load the data with\n","    outdir = f'{root}/data'\n","    if rank == 0 and not os.path.exists(outdir):\n","        train_data = datasets.MNIST(outdir, train=True,\n","                                   download=True, transform=transform)\n","        test_data = datasets.MNIST(outdir, train=False,\n","                                  download=True, transform=transform)\n","\n","    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n","    train_data = datasets.MNIST(outdir, train=True,\n","                                   download=True, transform=transform)\n","    test_data = datasets.MNIST(outdir, train=False,\n","                                  download=True, transform=transform)\n","\n","    ## create the validation split\n","    VALID_RATIO = 0.9\n","\n","    n_train_examples = int(len(train_data) * VALID_RATIO)\n","    n_valid_examples = len(train_data) - n_train_examples\n","\n","    train_data, valid_data = data.random_split(train_data,\n","                                              [n_train_examples, n_valid_examples])\n","\n","\n","    ## ensure the validation data uses the test transforms\n","    #valid_data = copy.deepcopy(valid_data)\n","    #valid_data.dataset.transform = test_transforms\n","\n","    ## print\n","    print(f'Number of training examples: {len(train_data)}')\n","    print(f'Number of validation examples: {len(valid_data)}')\n","    print(f'Number of testing examples: {len(test_data)}')\n","\n","\n","\n","    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n","    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n","\n","    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n","\n","\n","    return train_dataloader, val_dataloader, test_dataloader\n","\n","\"\"\"# 5. Implement the Training Loop\n","\n","## a. Help function\n","\"\"\"\n","\n","RESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n","\n","def log_results(scenario, results):\n","    \"\"\"\n","    Save results to a JSON file for comparison across scenarios.\n","    \"\"\"\n","    if os.path.exists(RESULTS_FILE):\n","        with open(RESULTS_FILE, 'r') as f:\n","            all_results = json.load(f)\n","    else:\n","        all_results = {}\n","\n","    all_results[scenario] = results\n","\n","    with open(RESULTS_FILE, 'w') as f:\n","        json.dump(all_results, f, indent=4)\n","\n","def calculate_accuracy(y_pred, y):\n","    top_pred = y_pred.argmax(1, keepdim=True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\"\"\"## b. train function\"\"\"\n","\n","def train(model, iterator, optimizer, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    i = 0\n","    for data in tqdm(iterator, desc= f\"Training on the rank {rank}\", leave=False):\n","        images, _ = data\n","        images = images.to(rank)\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        outputs = model(images)\n","        # calculate the loss\n","        loss = criterion(outputs, images)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update running training loss\n","        epoch_loss += loss.item()\n","\n","        if i % 50 == 0 and rank == 0:\n","            print(f\"- {i} was passed over {len(iterator)}\")\n","\n","        i+=1\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\"\"\"## c. Validation function\"\"\"\n","\n","def evaluate(model, iterator, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","    i=0\n","    with torch.no_grad():\n","\n","        for (images, labels) in tqdm(iterator, desc=f\"Evaluating on the rank {rank}\", leave=False):\n","\n","            images = images.to(rank)\n","            labels = labels.to(rank)\n","\n","            # get sample outputs\n","            output = model(images)\n","\n","            loss = criterion(output, images)\n","\n","            # acc = calculate_accuracy(y_pred, y)\n","\n","            epoch_loss += loss.item()\n","            # epoch_acc += acc.item()\n","            if i % 50 == 0 and rank == 0:\n","                print(f\"- {i} was passed over {len(iterator)}\")\n","            i+=1\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","outdir = f'{ROOT}/model/'\n","if not os.path.exists(outdir):\n","    os.makedirs(outdir)\n","\n","def main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n","    ## a. Set up the distributed process groups\n","    setup(rank, world_size)\n","    print(f\"Process {rank} initialized.\")\n","\n","    ## b. Create Model, DataLoader\n","    model = create_model().to(rank)  # Move model to GPU\n","    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n","\n","    ## c. Wrap the model with DistributedDataParallel\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    ## d. Loss and Optimizer\n","    criterion = nn.MSELoss().to(rank) # Move loss to GPU\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    ## e. Training Loop\n","    best_valid_loss = float('inf')\n","    training_times = []\n","    train_losses = []\n","    train_accurcy = []\n","    validation_times = []\n","    validation_losses = []\n","    validation_accurcy = []\n","\n","    epoch_times = []\n","\n","    for epoch in trange(num_epochs, desc=\"Epochs\"):\n","        start_epoch_time = time.monotonic()\n","        start_time = time.monotonic()\n","\n","        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n","        train_time = time.monotonic() - start_time\n","        training_times.append(train_time)\n","        train_losses.append(train_loss)\n","        train_accurcy.append(train_acc)\n","\n","        start_time = time.monotonic()\n","        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n","        val_time = time.monotonic() - start_time\n","        validation_times.append(val_time)\n","        validation_losses.append(valid_loss)\n","        validation_accurcy.append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(ddp_model.state_dict(), f'{root}tut4-model.pt')\n","\n","        end_time = time.monotonic()\n","        e_time = end_time - start_epoch_time\n","        epoch_times.append(e_time)\n","        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n","\n","        print(f'--------------|     On process {rank}      |----------------')\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ## f. test after train\n","    ddp_model.load_state_dict(torch.load(f'{root}tut4-model.pt'))\n","    start_time = time.monotonic()\n","    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank)\n","    test_time = time.monotonic() - start_time\n","    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # Log results\n","    results = {\n","        \"world_size\": world_size,\n","        \"rank\": rank,\n","        \"training_times\": training_times,\n","        \"train_losses\": train_losses,\n","        \"train_accurcy\": train_accurcy,\n","        \"validation_times\": validation_times,\n","        \"validation_losses\": validation_losses,\n","        \"validation_accurcy\": validation_accurcy,\n","        \"test_time\": test_time,\n","        \"test_loss\": test_loss,\n","        \"test_acc\": test_acc,\n","        \"epoch_times\": epoch_times\n","     }\n","\n","    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n","    log_results(scenario, results)\n","    dist.barrier()\n","\n","    cleanup()\n","    print(f'Process {rank} finished training.')\n","\n","\"\"\"# 6. Main Execution\"\"\"\n","if __name__ == \"__main__\":\n","\n","    def main():\n","        world_size = torch.cuda.device_count()\n","        print(f'Total number of devices detected: {world_size}')\n","\n","        if world_size >= 1:\n","            #start the training process on all available GPUs\n","            if world_size > 1:\n","                #start the training process on all available GPUs\n","                mp.spawn(\n","                    main_train,\n","                    args=(world_size,),\n","                    nprocs=world_size,\n","                    join=True\n","                )\n","            else:\n","                #run training on single GPU\n","                main_train(rank=0, world_size=1)\n","\n","        else:\n","            print('no GPUs found. Please make sure you have configured CUDA correctly')\n","\n","    main()"]},{"cell_type":"code","source":["!python main.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GFW4IaRqxD_p","executionInfo":{"status":"ok","timestamp":1736438402944,"user_tz":-60,"elapsed":129417,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"02a5ce7d-9177-41c3-a97b-c3c03f2b1800"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of devices detected: 1\n","Process 0 initialized.\n","The model has 1,077 trainable parameters\n","[rank0]:[W109 15:57:55.492580377 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n","Number of training examples: 54000\n","Number of validation examples: 6000\n","Number of testing examples: 10000\n","Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 01 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.050 | Train Acc: 0.00%\n","\t Val. Loss: 0.030 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 02 | Epoch Time: 0m 11s\n","\tTrain Loss: 0.029 | Train Acc: 0.00%\n","\t Val. Loss: 0.027 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 03 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.025 | Train Acc: 0.00%\n","\t Val. Loss: 0.023 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 04 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.023 | Train Acc: 0.00%\n","\t Val. Loss: 0.022 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 05 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.022 | Train Acc: 0.00%\n","\t Val. Loss: 0.021 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 06 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.021 | Train Acc: 0.00%\n","\t Val. Loss: 0.021 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 07 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.021 | Train Acc: 0.00%\n","\t Val. Loss: 0.021 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 08 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.021 | Train Acc: 0.00%\n","\t Val. Loss: 0.020 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 09 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.020 | Train Acc: 0.00%\n","\t Val. Loss: 0.020 |  Val. Acc: 0.00%\n","Training on the rank 0:   0%|          | 0/1688 [00:00<?, ?it/s]\n","- 0 was passed over 1688\n","- 50 was passed over 1688\n","- 100 was passed over 1688\n","- 150 was passed over 1688\n","- 200 was passed over 1688\n","- 250 was passed over 1688\n","- 300 was passed over 1688\n","- 350 was passed over 1688\n","- 400 was passed over 1688\n","- 450 was passed over 1688\n","- 500 was passed over 1688\n","- 550 was passed over 1688\n","- 600 was passed over 1688\n","- 650 was passed over 1688\n","- 700 was passed over 1688\n","- 750 was passed over 1688\n","- 800 was passed over 1688\n","- 850 was passed over 1688\n","- 900 was passed over 1688\n","- 950 was passed over 1688\n","- 1000 was passed over 1688\n","- 1050 was passed over 1688\n","- 1100 was passed over 1688\n","- 1150 was passed over 1688\n","- 1200 was passed over 1688\n","- 1250 was passed over 1688\n","- 1300 was passed over 1688\n","- 1350 was passed over 1688\n","- 1400 was passed over 1688\n","- 1450 was passed over 1688\n","- 1500 was passed over 1688\n","- 1550 was passed over 1688\n","- 1600 was passed over 1688\n","- 1650 was passed over 1688\n","Evaluating on the rank 0:   0%|          | 0/188 [00:00<?, ?it/s]\n","- 0 was passed over 188\n","- 50 was passed over 188\n","- 100 was passed over 188\n","- 150 was passed over 188\n","--------------|     On process 0      |----------------\n","Epoch: 10 | Epoch Time: 0m 12s\n","\tTrain Loss: 0.020 | Train Acc: 0.00%\n","\t Val. Loss: 0.020 |  Val. Acc: 0.00%\n","/content/main.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ddp_model.load_state_dict(torch.load(f'{root}tut4-model.pt'))\n","Evaluating on the rank 0:   0%|          | 0/313 [00:00<?, ?it/s]\n","- 0 was passed over 313\n","- 50 was passed over 313\n","- 100 was passed over 313\n","- 150 was passed over 313\n","- 200 was passed over 313\n","- 250 was passed over 313\n","- 300 was passed over 313\n","Test results on process 0: Test Loss: 0.020 | Test Acc: 0.00%\n","Process 0 finished training.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wYTgjjAJxGXy","executionInfo":{"status":"ok","timestamp":1736438402945,"user_tz":-60,"elapsed":8,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}}},"execution_count":6,"outputs":[]}]}