{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM6djcEvLNrc5WHVrQu+CED"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJGWGqiN4obW","executionInfo":{"status":"ok","timestamp":1736382326936,"user_tz":-60,"elapsed":428,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"81713844-95e9-43a6-f315-20f668dea536"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}],"source":["%%writefile main.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","from sklearn import metrics\n","from sklearn import decomposition\n","from sklearn import manifold\n","from tqdm.notebook import trange, tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","import copy\n","import random\n","import time\n","import os\n","import json\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","SEED = 1234\n","ROOT = \".\"\n","MODEL_NAME = \"MLP\"\n","SENARIO = \"1GPU\"\n","EPOCHS = 10\n","BATCH_SIZE = 512\n","\n","outdir = \"./my_datasets\"\n","os.makedirs(outdir, exist_ok=True)\n","os.environ['HF_DATASETS_CACHE'] = outdir\n","\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","\"\"\"# 2. Initialize the DDP Environment\"\"\"\n","\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n","    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()\n","\n","\"\"\"# 3. Define a Model.\"\"\"\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","\n","        self.input_fc = nn.Linear(input_dim, 250)\n","        self.hidden_fc = nn.Linear(250, 100)\n","        self.output_fc = nn.Linear(100, output_dim)\n","\n","    def forward(self, x):\n","\n","        # x = [batch size, height, width]\n","\n","        batch_size = x.shape[0]\n","\n","        x = x.view(batch_size, -1)\n","\n","        # x = [batch size, height * width]\n","\n","        h_1 = F.relu(self.input_fc(x))\n","\n","        # h_1 = [batch size, 250]\n","\n","        h_2 = F.relu(self.hidden_fc(h_1))\n","\n","        # h_2 = [batch size, 100]\n","\n","        y_pred = self.output_fc(h_2)\n","\n","        # y_pred = [batch size, output dim]\n","\n","        return y_pred, h_2\n","\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def create_model():\n","    INPUT_DIM = 28 * 28\n","    OUTPUT_DIM = 10\n","\n","    model = MLP(INPUT_DIM, OUTPUT_DIM)\n","    print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","    return model\n","\n","\"\"\"# 4. Create a Dummy Dataset\"\"\"\n","\n","def create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n","    mean = 0.13066047430038452\n","    std = 0.30810779333114624\n","\n","    train_transforms = transforms.Compose([\n","                            transforms.RandomRotation(5, fill=(0,)),\n","                            transforms.RandomCrop(28, padding=2),\n","                            transforms.ToTensor(),\n","                            transforms.Normalize(mean=[mean], std=[std])\n","                                      ])\n","\n","    test_transforms = transforms.Compose([\n","                           transforms.ToTensor(),\n","                           transforms.Normalize(mean=[mean], std=[std])\n","                                     ])\n","\n","    ## load the data with\n","    outdir = f\"{root}/data\"\n","    if rank == 0 and not os.path.exists(outdir):\n","        train_data = datasets.MNIST(root=outdir,\n","                                    train=True,\n","                                    download=True,\n","                                    transform=train_transforms)\n","\n","        test_data = datasets.MNIST(root=outdir,\n","                                  train=False,\n","                                  download=True,\n","                                  transform=test_transforms)\n","\n","    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n","\n","    train_data = datasets.MNIST(root=outdir,\n","                                train=True,\n","                                download=True,\n","                                transform=train_transforms)\n","\n","    test_data = datasets.MNIST(root=outdir,\n","                              train=False,\n","                              download=True,\n","                              transform=test_transforms)\n","\n","\n","    ## create the validation split\n","    VALID_RATIO = 0.9\n","\n","    n_train_examples = int(len(train_data) * VALID_RATIO)\n","    n_valid_examples = len(train_data) - n_train_examples\n","    train_data, valid_data = data.random_split(train_data,\n","                                           [n_train_examples, n_valid_examples])\n","\n","    if rank == 0:\n","        print(f'Number of training examples: {len(train_data)}')\n","        print(f'Number of validation examples: {len(valid_data)}')\n","        print(f'Number of testing examples: {len(test_data)}')\n","\n","\n","    ## Creating Data Loaders\n","\n","    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n","    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n","\n","    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n","    return train_dataloader, val_dataloader, test_dataloader\n","\n","\"\"\"# 5. Implement the Training Loop\n","\n","## a. Help function\n","\"\"\"\n","\n","RESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n","\n","def log_results(scenario, results):\n","    \"\"\"\n","    Save results to a JSON file for comparison across scenarios.\n","    \"\"\"\n","    if os.path.exists(RESULTS_FILE):\n","        with open(RESULTS_FILE, 'r') as f:\n","            all_results = json.load(f)\n","    else:\n","        all_results = {}\n","\n","    all_results[scenario] = results\n","\n","    with open(RESULTS_FILE, 'w') as f:\n","        json.dump(all_results, f, indent=4)\n","\n","def calculate_accuracy(y_pred, y):\n","    top_pred = y_pred.argmax(1, keepdim=True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\"\"\"## b. train function\"\"\"\n","def train(model, iterator, optimizer, criterion, rank):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    i=0\n","    for (x, y) in tqdm(iterator, desc=f\"Training on the rank {rank}...\", leave=False):\n","\n","        x = x.to(rank)\n","        y = y.to(rank)\n","\n","        optimizer.zero_grad()\n","\n","        y_pred, _ = model(x)\n","\n","        loss = criterion(y_pred, y)\n","\n","        acc = calculate_accuracy(y_pred, y)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        if i % 50 == 0 and rank == 0 :\n","            print(f\"- On Training: {i} was passed over  {len(iterator)}\")\n","        i+=1\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\n","\"\"\"## c. Validation function\"\"\"\n","def evaluate(model, iterator, criterion, rank, mode = \"Evaluating\"):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","    i=0\n","    with torch.no_grad():\n","\n","        for (x, y) in tqdm(iterator, desc=f\"{mode} on the rank {rank} ...\", leave=False):\n","\n","            x = x.to(rank)\n","            y = y.to(rank)\n","\n","            y_pred, _ = model(x)\n","\n","            loss = criterion(y_pred, y)\n","\n","            acc = calculate_accuracy(y_pred, y)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","            if i % 50 == 0 and rank == 0:\n","                print(f\"- On {mode}: {i} was passed over  {len(iterator)}\")\n","            i+=1\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","\n","\"\"\"## d. Main loop\"\"\"\n","\n","outdir = f'{ROOT}/model/'\n","if not os.path.exists(outdir):\n","    os.makedirs(outdir)\n","\n","def main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n","    ## a. Set up the distributed process groups\n","    setup(rank, world_size)\n","    print(f\"Process {rank} initialized.\")\n","\n","    # setup mp_model and devices for this process\n","\n","\n","    ## b. Create Model, DataLoader\n","    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n","    model = create_model().to(rank)\n","\n","    ## c. Wrap the model with DistributedDataParallel\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    ## d. Loss and Optimizer\n","    #LR = 5e-4\n","    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n","    optimizer = optim.Adam(ddp_model.parameters())\n","\n","    ## e. Training Loop\n","    best_valid_loss = float('inf')\n","    training_times = []\n","    train_losses = []\n","    train_accurcy = []\n","    validation_times = []\n","    validation_losses = []\n","    validation_accurcy = []\n","\n","    epoch_times = []\n","\n","    for epoch in trange(num_epochs, desc=\"Epochs\"):\n","        start_epoch_time = time.monotonic()\n","        start_time = time.monotonic()\n","\n","        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n","        train_time = time.monotonic() - start_time\n","        training_times.append(train_time)\n","        train_losses.append(train_loss)\n","        train_accurcy.append(train_acc)\n","\n","        start_time = time.monotonic()\n","        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n","        val_time = time.monotonic() - start_time\n","        validation_times.append(val_time)\n","        validation_losses.append(valid_loss)\n","        validation_accurcy.append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(ddp_model.state_dict(), f'{root}mlp-model.pt')\n","\n","        end_time = time.monotonic()\n","        e_time = end_time - start_epoch_time\n","        epoch_times.append(e_time)\n","        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n","\n","        print(f'--------------|     On process {rank}      |----------------')\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ## f. test after train\n","    ddp_model.load_state_dict(torch.load(f'{root}mlp-model.pt'))\n","    start_time = time.monotonic()\n","    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n","    test_time = time.monotonic() - start_time\n","    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # Log results\n","    results = {\n","        \"world_size\": world_size,\n","        \"rank\": rank,\n","        \"training_times\": training_times,\n","        \"train_losses\": train_losses,\n","        \"train_accurcy\": train_accurcy,\n","        \"validation_times\": validation_times,\n","        \"validation_losses\": validation_losses,\n","        \"validation_accurcy\": validation_accurcy,\n","        \"test_time\": test_time,\n","        \"test_loss\": test_loss,\n","        \"test_acc\": test_acc,\n","        \"epoch_times\": epoch_times\n","     }\n","\n","    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n","    log_results(scenario, results)\n","    dist.barrier()\n","\n","    cleanup()\n","    print(f'Process {rank} finished training.')\n","\n","\"\"\"# 6. Main Execution\"\"\"\n","if __name__ == \"__main__\":\n","\n","    def main():\n","        world_size = torch.cuda.device_count()\n","        print(f'Total number of devices detected: {world_size}')\n","\n","        if world_size >= 1:\n","            #start the training process on all available GPUs\n","\n","            if world_size > 1:\n","                #start the training process on all available GPUs\n","\n","                mp.spawn(\n","                    main_train,\n","                    args=(world_size,),\n","                    nprocs=world_size,\n","                    join=True\n","                )\n","            else:\n","                #run training on single GPU\n","                main_train(rank=0, world_size=1)\n","\n","        else:\n","            print('no GPUs found. Please make sure you have configured CUDA correctly')\n","\n","    main()"]},{"cell_type":"code","source":["!python main.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVHrQjLxTicQ","executionInfo":{"status":"ok","timestamp":1736382634825,"user_tz":-60,"elapsed":307308,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"d976d32e-450c-4703-92c2-6ac4aaab03a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of devices detected: 1\n","Process 0 initialized.\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","100% 9.91M/9.91M [00:02<00:00, 4.51MB/s]\n","Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","100% 28.9k/28.9k [00:00<00:00, 134kB/s]\n","Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","100% 1.65M/1.65M [00:06<00:00, 247kB/s]\n","Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","100% 4.54k/4.54k [00:00<00:00, 15.5MB/s]\n","Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","[rank0]:[W109 00:25:58.585172854 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n","Number of training examples: 54000\n","Number of validation examples: 6000\n","Number of testing examples: 10000\n","The model has 222,360 trainable parameters\n","Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 01 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.792 | Train Acc: 75.82%\n","\t Val. Loss: 0.414 |  Val. Acc: 87.49%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 02 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.297 | Train Acc: 91.10%\n","\t Val. Loss: 0.257 |  Val. Acc: 92.55%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 03 | Epoch Time: 0m 28s\n","\tTrain Loss: 0.206 | Train Acc: 93.91%\n","\t Val. Loss: 0.197 |  Val. Acc: 94.43%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 04 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.166 | Train Acc: 95.01%\n","\t Val. Loss: 0.177 |  Val. Acc: 94.39%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 05 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.146 | Train Acc: 95.52%\n","\t Val. Loss: 0.149 |  Val. Acc: 95.54%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 06 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.129 | Train Acc: 96.08%\n","\t Val. Loss: 0.139 |  Val. Acc: 95.51%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 07 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.117 | Train Acc: 96.45%\n","\t Val. Loss: 0.136 |  Val. Acc: 95.97%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 08 | Epoch Time: 0m 27s\n","\tTrain Loss: 0.109 | Train Acc: 96.53%\n","\t Val. Loss: 0.123 |  Val. Acc: 96.17%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 09 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.099 | Train Acc: 96.87%\n","\t Val. Loss: 0.106 |  Val. Acc: 96.90%\n","Training on the rank 0...:   0%|          | 0/106 [00:00<?, ?it/s]\n","- On Training: 0 was passed over  106\n","- On Training: 50 was passed over  106\n","- On Training: 100 was passed over  106\n","Evaluating on the rank 0 ...:   0%|          | 0/12 [00:00<?, ?it/s]\n","- On Evaluating: 0 was passed over  12\n","--------------|     On process 0      |----------------\n","Epoch: 10 | Epoch Time: 0m 26s\n","\tTrain Loss: 0.095 | Train Acc: 97.04%\n","\t Val. Loss: 0.111 |  Val. Acc: 96.78%\n","/content/main.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  ddp_model.load_state_dict(torch.load(f'{root}mlp-model.pt'))\n","Testing on the rank 0 ...:   0%|          | 0/20 [00:00<?, ?it/s]\n","- On Testing: 0 was passed over  20\n","Test results on process 0: Test Loss: 0.066 | Test Acc: 98.06%\n","Process 0 finished training.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LlmPzYpTTq0g"},"execution_count":null,"outputs":[]}]}