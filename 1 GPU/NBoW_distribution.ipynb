{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM1UpqKbnFV0bEZR53QAgMF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"YXen6lHtdvJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchtext==0.17.0 torch==2.2.0"],"metadata":{"id":"esiITo5kc3vq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NStV1qSJkbF","executionInfo":{"status":"ok","timestamp":1736371611793,"user_tz":-60,"elapsed":490,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"94e28d82-6a12-4f4f-bf4b-a22b9d5243cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting main.py\n"]}],"source":["%%writefile main.py\n","\n","import datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchtext\n","\n","import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import copy\n","import random\n","import time\n","import os\n","import json\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","import torch.distributed as dist\n","import torch.multiprocessing as mp\n","\n","SEED = 1234\n","ROOT = \".\"\n","MODEL_NAME = \"NBoW\"\n","SENARIO = \"1GPU\"\n","EPOCHS = 5\n","BATCH_SIZE = 512\n","\n","outdir = \"./my_datasets\"\n","os.makedirs(outdir, exist_ok=True)\n","os.environ['HF_DATASETS_CACHE'] = outdir\n","\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","\"\"\"# 2. Initialize the DDP Environment\"\"\"\n","\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n","    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n","    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n","\n","def cleanup():\n","    dist.destroy_process_group()\n","\n","\"\"\"# 3. Define a Model.\"\"\"\n","\n","class NBoW(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, ids):\n","        # ids = [batch size, seq len]\n","        embedded = self.embedding(ids)\n","        # embedded = [batch size, seq len, embedding dim]\n","        pooled = embedded.mean(dim=1)\n","        # pooled = [batch size, embedding dim]\n","        prediction = self.fc(pooled)\n","        # prediction = [batch size, output dim]\n","        return prediction\n","\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def create_model(vocab, output_dim, pad_index, embedding_dim = 300):\n","    vocab_size = len(vocab)\n","    model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)\n","    print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","    vectors = torchtext.vocab.GloVe()\n","    pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n","\n","    model.embedding.weight.data = pretrained_embedding\n","\n","    return model\n","\n","\"\"\"# 4. Create a Dummy Dataset\"\"\"\n","\n","def create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n","    def tokenize_example(example, tokenizer, max_length):\n","        tokens = tokenizer(example[\"text\"])[:max_length]\n","        return {\"tokens\": tokens}\n","\n","    ## load the data with\n","    if rank == 0:\n","        # Load the dataset\n","        train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n","\n","    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n","\n","    train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n","\n","    ## Tokenization\n","    tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n","    train_data = train_data.map(\n","        tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n","    )\n","    test_data = test_data.map(\n","        tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n","    )\n","\n","    ## create the validation split\n","    test_size = 0.25\n","\n","    train_valid_data = test_data.train_test_split(test_size=test_size)\n","    test_data = train_valid_data[\"train\"]\n","    valid_data = train_valid_data[\"test\"]\n","\n","\n","    ## Creating a Vocabulary\n","    min_freq = 5\n","    special_tokens = [\"<unk>\", \"<pad>\"]\n","\n","    vocab = torchtext.vocab.build_vocab_from_iterator(\n","        train_data[\"tokens\"],\n","        min_freq=min_freq,\n","        specials=special_tokens,\n","    )\n","\n","    if rank == 0:\n","        print(f\"Vocabulary size: {len(vocab)}\")\n","        print(f'Number of training examples: {len(train_data)}')\n","        print(f'Number of validation examples: {len(valid_data)}')\n","        print(f'Number of testing examples: {len(test_data)}')\n","\n","    unk_index = vocab[\"<unk>\"]\n","    pad_index = vocab[\"<pad>\"]\n","    vocab.set_default_index(unk_index)\n","\n","    ## Numericalizing Data\n","    def numericalize_example(example, vocab):\n","        ids = vocab.lookup_indices(example[\"tokens\"])\n","        return {\"ids\": ids}\n","\n","    train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n","    valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n","    test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n","\n","    train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n","    valid_data = valid_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n","    test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n","\n","    ## Creating Data Loaders\n","    def get_collate_fn(pad_index):\n","        def collate_fn(batch):\n","            batch_ids = [i[\"ids\"] for i in batch]\n","            batch_ids = nn.utils.rnn.pad_sequence(\n","                batch_ids, padding_value=pad_index, batch_first=True\n","            )\n","            batch_label = [i[\"label\"] for i in batch]\n","            batch_label = torch.stack(batch_label)\n","            batch = {\"ids\": batch_ids, \"label\": batch_label}\n","            return batch\n","\n","        return collate_fn\n","\n","    collate_fn = get_collate_fn(pad_index)\n","    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n","    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n","\n","    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n","    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, pin_memory=True) #no sampling for test dataset\n","\n","\n","    output_dim = len(train_data.unique(\"label\"))\n","    return train_dataloader, val_dataloader, test_dataloader, vocab, output_dim, pad_index\n","\n","\"\"\"# 5. Implement the Training Loop\n","\n","## a. Help function\n","\"\"\"\n","\n","RESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n","\n","def log_results(scenario, results):\n","    \"\"\"\n","    Save results to a JSON file for comparison across scenarios.\n","    \"\"\"\n","    if os.path.exists(RESULTS_FILE):\n","        with open(RESULTS_FILE, 'r') as f:\n","            all_results = json.load(f)\n","    else:\n","        all_results = {}\n","\n","    all_results[scenario] = results\n","\n","    with open(RESULTS_FILE, 'w') as f:\n","        json.dump(all_results, f, indent=4)\n","\n","def get_accuracy(prediction, label):\n","    batch_size, _ = prediction.shape\n","    predicted_classes = prediction.argmax(dim=-1)\n","    correct_predictions = predicted_classes.eq(label).sum()\n","    accuracy = correct_predictions / batch_size\n","    return accuracy\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","\"\"\"## b. train function\"\"\"\n","def train(model, data_loader, criterion, optimizer, rank):\n","    model.train()\n","    epoch_losses = []\n","    epoch_accs = []\n","    i=0\n","    for batch in tqdm.tqdm(data_loader, desc=f\"Training on the rank {rank}...\"):\n","        ids = batch[\"ids\"].to(rank)\n","        label = batch[\"label\"].to(rank)\n","        prediction = model(ids)\n","        loss = criterion(prediction, label)\n","        accuracy = get_accuracy(prediction, label)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        epoch_losses.append(loss.item())\n","        epoch_accs.append(accuracy.item())\n","        if i % 50 == 0 and rank == 0:\n","            print(f\"- On Training: {i} was passed over  {len(data_loader)}\")\n","        i+=1\n","    return np.mean(epoch_losses), np.mean(epoch_accs)\n","\n","\"\"\"## c. Validation function\"\"\"\n","\n","def evaluate(model, data_loader, criterion, rank, mode = \"Evaluating\"):\n","    model.eval()\n","    epoch_losses = []\n","    epoch_accs = []\n","    i = 0\n","    with torch.no_grad():\n","        for batch in tqdm.tqdm(data_loader, desc=f\"{mode} on the rank {rank}...\"):\n","            ids = batch[\"ids\"].to(rank)\n","            label = batch[\"label\"].to(rank)\n","            prediction = model(ids)\n","            loss = criterion(prediction, label)\n","            accuracy = get_accuracy(prediction, label)\n","            epoch_losses.append(loss.item())\n","            epoch_accs.append(accuracy.item())\n","            if i % 50 == 0 and rank == 0:\n","                print(f\"- On {mode}: {i} was passed over  {len(data_loader)}\")\n","            i+=1\n","    return np.mean(epoch_losses), np.mean(epoch_accs)\n","\n","\"\"\"## d. Main loop\"\"\"\n","\n","outdir = f'{ROOT}/model/'\n","if not os.path.exists(outdir):\n","    os.makedirs(outdir)\n","\n","def main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n","    ## a. Set up the distributed process groups\n","    setup(rank, world_size)\n","    print(f\"Process {rank} initialized.\")\n","\n","    ## b. Create Model, DataLoader\n","    train_dataloader, val_dataloader, test_dataloader, vocab, output_dim, pad_index = create_dataloader(rank, world_size)\n","    model = create_model(vocab, output_dim, pad_index).to(rank)\n","\n","    ## c. Wrap the model with DistributedDataParallel\n","    ddp_model = DDP(model, device_ids=[rank])\n","\n","    ## d. Loss and Optimizer\n","    #LR = 5e-4\n","    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n","    optimizer = optim.Adam(ddp_model.parameters())\n","\n","    ## e. Training Loop\n","    best_valid_loss = float('inf')\n","    training_times = []\n","    train_losses = []\n","    train_accurcy = []\n","    validation_times = []\n","    validation_losses = []\n","    validation_accurcy = []\n","\n","    epoch_times = []\n","\n","    for epoch in range(num_epochs):\n","        start_epoch_time = time.monotonic()\n","        start_time = time.monotonic()\n","\n","        train_loss, train_acc = train(ddp_model, train_dataloader, criterion, optimizer, rank)\n","        train_time = time.monotonic() - start_time\n","        training_times.append(train_time)\n","        train_losses.append(train_loss)\n","        train_accurcy.append(train_acc)\n","\n","        start_time = time.monotonic()\n","        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n","        val_time = time.monotonic() - start_time\n","        validation_times.append(val_time)\n","        validation_losses.append(valid_loss)\n","        validation_accurcy.append(valid_acc)\n","\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(ddp_model.state_dict(), f'{root}tut-model.pt')\n","\n","        end_time = time.monotonic()\n","        e_time = end_time - start_epoch_time\n","        epoch_times.append(e_time)\n","        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n","\n","        print(f'--------------|     On process {rank}      |----------------')\n","        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    ## f. test after train\n","    ddp_model.load_state_dict(torch.load(f'{root}tut-model.pt'))\n","    start_time = time.monotonic()\n","    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n","    test_time = time.monotonic() - start_time\n","    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # Log results\n","    results = {\n","        \"world_size\": world_size,\n","        \"rank\": rank,\n","        \"training_times\": training_times,\n","        \"train_losses\": train_losses,\n","        \"train_accurcy\": train_accurcy,\n","        \"validation_times\": validation_times,\n","        \"validation_losses\": validation_losses,\n","        \"validation_accurcy\": validation_accurcy,\n","        \"test_time\": test_time,\n","        \"test_loss\": test_loss,\n","        \"test_acc\": test_acc,\n","        \"epoch_times\": epoch_times\n","     }\n","\n","    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n","    log_results(scenario, results)\n","    dist.barrier()\n","\n","    cleanup()\n","    print(f'Process {rank} finished training.')\n","\n","\"\"\"# 6. Main Execution\"\"\"\n","if __name__ == \"__main__\":\n","\n","    def main():\n","        world_size = torch.cuda.device_count()\n","        print(f'Total number of devices detected: {world_size}')\n","\n","        if world_size >= 1:\n","            #start the training process on all available GPUs\n","            if world_size > 1:\n","                #start the training process on all available GPUs\n","                mp.spawn(\n","                    main_train,\n","                    args=(world_size,),\n","                    nprocs=world_size,\n","                    join=True\n","                )\n","            else:\n","                #run training on single GPU\n","                main_train(rank=0, world_size=1)\n","\n","        else:\n","            print('no GPUs found. Please make sure you have configured CUDA correctly')\n","\n","    main()"]},{"cell_type":"code","source":["!python main.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9KrmjD_qr-j","executionInfo":{"status":"ok","timestamp":1736371654385,"user_tz":-60,"elapsed":42600,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}},"outputId":"dd1748a2-03fc-4c73-b7d5-14e42298d2f0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of devices detected: 1\n","Process 0 initialized.\n","Vocabulary size: 24897\n","Number of training examples: 25000\n","Number of validation examples: 6250\n","Number of testing examples: 18750\n","The model has 7,469,702 trainable parameters\n","Training on the rank 0...:   0% 0/49 [00:00<?, ?it/s]- On Training: 0 was passed over  49\n","Training on the rank 0...: 100% 49/49 [00:02<00:00, 21.24it/s]\n","Evaluating on the rank 0...:   0% 0/13 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  13\n","Evaluating on the rank 0...: 100% 13/13 [00:00<00:00, 37.88it/s]\n","--------------|     On process 0      |----------------\n","Epoch: 01 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.677 | Train Acc: 66.89%\n","\t Val. Loss: 0.655 |  Val. Acc: 72.07%\n","Training on the rank 0...:   0% 0/49 [00:00<?, ?it/s]- On Training: 0 was passed over  49\n","Training on the rank 0...: 100% 49/49 [00:01<00:00, 37.51it/s]\n","Evaluating on the rank 0...:   0% 0/13 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  13\n","Evaluating on the rank 0...: 100% 13/13 [00:00<00:00, 52.39it/s]\n","--------------|     On process 0      |----------------\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.616 | Train Acc: 75.35%\n","\t Val. Loss: 0.586 |  Val. Acc: 75.71%\n","Training on the rank 0...:   0% 0/49 [00:00<?, ?it/s]- On Training: 0 was passed over  49\n","Training on the rank 0...: 100% 49/49 [00:01<00:00, 29.85it/s]\n","Evaluating on the rank 0...:   0% 0/13 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  13\n","Evaluating on the rank 0...: 100% 13/13 [00:00<00:00, 35.04it/s]\n","--------------|     On process 0      |----------------\n","Epoch: 03 | Epoch Time: 0m 2s\n","\tTrain Loss: 0.528 | Train Acc: 80.39%\n","\t Val. Loss: 0.509 |  Val. Acc: 79.73%\n","Training on the rank 0...:   0% 0/49 [00:00<?, ?it/s]- On Training: 0 was passed over  49\n","Training on the rank 0...: 100% 49/49 [00:01<00:00, 30.56it/s]\n","Evaluating on the rank 0...:   0% 0/13 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  13\n","Evaluating on the rank 0...: 100% 13/13 [00:00<00:00, 52.29it/s]\n","--------------|     On process 0      |----------------\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.441 | Train Acc: 84.42%\n","\t Val. Loss: 0.448 |  Val. Acc: 82.71%\n","Training on the rank 0...:   0% 0/49 [00:00<?, ?it/s]- On Training: 0 was passed over  49\n","Training on the rank 0...: 100% 49/49 [00:01<00:00, 36.17it/s]\n","Evaluating on the rank 0...:   0% 0/13 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  13\n","Evaluating on the rank 0...: 100% 13/13 [00:00<00:00, 52.33it/s]\n","--------------|     On process 0      |----------------\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.375 | Train Acc: 87.09%\n","\t Val. Loss: 0.408 |  Val. Acc: 83.89%\n","Testing on the rank 0...:   0% 0/37 [00:00<?, ?it/s]- On Testing: 0 was passed over  37\n","Testing on the rank 0...: 100% 37/37 [00:00<00:00, 43.62it/s]\n","Test results on process 0: Test Loss: 0.408 | Test Acc: 83.96%\n","Process 0 finished training.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b0uy7Q_Mrw9G","executionInfo":{"status":"ok","timestamp":1736371654385,"user_tz":-60,"elapsed":7,"user":{"displayName":"Mohamed STIFI","userId":"12568271599600391156"}}},"execution_count":19,"outputs":[]}]}