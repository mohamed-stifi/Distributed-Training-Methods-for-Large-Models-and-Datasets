{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchtext==0.17.0 torch==2.2.0\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:10:32.741676Z","iopub.execute_input":"2025-01-10T08:10:32.742032Z"}},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\n!python -m spacy download de_core_news_sm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport spacy\nimport datasets\nimport torchtext\nimport tqdm\n\n\n\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\nimport json\n\nimport torch.utils.data as data\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nSEED = 1234\nROOT = \".\"\nMODEL_NAME = \"seq2seq\"\nSENARIO = \"2GPU\"\nEPOCHS = 10\nCLIP = 1.0\nteacher_forcing_ratio = 0.5\nBATCH_SIZE = 512\n\noutdir = \"./my_datasets\"\nos.makedirs(outdir, exist_ok=True)\nos.environ['HF_DATASETS_CACHE'] = outdir\n\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\"\"\"# 2. Initialize the DDP Environment\"\"\"\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"# 3. Define a Model.\"\"\"\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        # src = [src length, batch size]\n        embedded = self.dropout(self.embedding(src))\n        # embedded = [src length, batch size, embedding dim]\n        outputs, hidden = self.rnn(embedded)  # no cell state in GRU!\n        # outputs = [src length, batch size, hidden dim * n directions]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # outputs are always from the top hidden layer\n        return hidden\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, hidden_dim, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        self.rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim)\n        self.fc_out = nn.Linear(embedding_dim + hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, context):\n        # input = [batch size]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # context = [n layers * n directions, batch size, hidden dim]\n        # n layers and n directions in the decoder will both always be 1, therefore:\n        # hidden = [1, batch size, hidden dim]\n        # context = [1, batch size, hidden dim]\n        input = input.unsqueeze(0)\n        # input = [1, batch size]\n        embedded = self.dropout(self.embedding(input))\n        # embedded = [1, batch size, embedding dim]\n        emb_con = torch.cat((embedded, context), dim=2)\n        # emb_con = [1, batch size, embedding dim + hidden dim]\n        output, hidden = self.rnn(emb_con, hidden)\n        # output = [seq len, batch size, hidden dim * n directions]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n        # output = [1, batch size, hidden dim]\n        # hidden = [1, batch size, hidden dim]\n        output = torch.cat(\n            (embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1\n        )\n        # output = [batch size, embedding dim + hidden dim * 2]\n        prediction = self.fc_out(output)\n        # prediction = [batch size, output dim]\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        # self.device = device\n        assert (\n            encoder.hidden_dim == decoder.hidden_dim\n        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n\n    def forward(self, src, trg, teacher_forcing_ratio):\n        # src = [src length, batch size]\n        # trg = [trg length, batch size]\n        # teacher_forcing_ratio is probability to use teacher forcing\n        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        batch_size = trg.shape[1]\n        trg_length = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        # tensor to store decoder outputs\n        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(trg.device)\n        # last hidden state of the encoder is the context\n        context = self.encoder(src)\n        # context = [n layers * n directions, batch size, hidden dim]\n        # context also used as the initial hidden state of the decoder\n        hidden = context\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # first input to the decoder is the <sos> tokens\n        input = trg[0, :]\n        for t in range(1, trg_length):\n            # insert input token embedding, previous hidden state and the context state\n            # receive output tensor (predictions) and new hidden state\n            output, hidden = self.decoder(input, hidden, context)\n            # output = [batch size, output dim]\n            # hidden = [1, batch size, hidden dim]\n            # place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            # decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            # get the highest predicted token from our predictions\n            top1 = output.argmax(1)\n            # if teacher forcing, use actual next token as next input\n            # if not, use predicted token\n            input = trg[t] if teacher_force else top1\n            # input = [batch size]\n        return outputs\n\n\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n\ndef create_model(de_vocab, en_vocab):\n\n\n    input_dim = len(de_vocab)\n    output_dim = len(en_vocab)\n    encoder_embedding_dim = 256\n    decoder_embedding_dim = 256\n    hidden_dim = 512\n    n_layers = 2\n    encoder_dropout = 0.5\n    decoder_dropout = 0.5\n\n    encoder = Encoder(\n        input_dim,\n        encoder_embedding_dim,\n        hidden_dim,\n        n_layers,\n        encoder_dropout,\n    )\n\n    decoder = Decoder(\n        output_dim,\n        decoder_embedding_dim,\n        hidden_dim,\n        n_layers,\n        decoder_dropout,\n    )\n\n    model = Seq2Seq(encoder, decoder)\n    print(f'The model has {count_parameters(model):,} trainable parameters')\n    model.apply(init_weights)\n    return model\n\n\"\"\"# 4. Create a Dummy Dataset\"\"\"\n\ndef create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n    def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n        en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n        de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n        if lower:\n            en_tokens = [token.lower() for token in en_tokens]\n            de_tokens = [token.lower() for token in de_tokens]\n        en_tokens = [sos_token] + en_tokens + [eos_token]\n        de_tokens = [sos_token] + de_tokens + [eos_token]\n        return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n    ## load the data with\n    if rank == 0:\n        dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n\n    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n\n    dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n\n    train_data, valid_data, test_data = (\n        dataset[\"train\"],\n        dataset[\"validation\"],\n        dataset[\"test\"],\n    )\n\n    ## Tokenization\n    en_nlp = spacy.load(\"en_core_web_sm\")\n    de_nlp = spacy.load(\"de_core_news_sm\")\n\n    max_length = 1_000\n    lower = True\n    sos_token = \"<sos>\"\n    eos_token = \"<eos>\"\n    \n    fn_kwargs = {\n        \"en_nlp\": en_nlp,\n        \"de_nlp\": de_nlp,\n        \"max_length\": max_length,\n        \"lower\": lower,\n        \"sos_token\": sos_token,\n        \"eos_token\": eos_token,\n    }\n    \n    train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n    valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n    test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n\n    ## create the validation split\n\n\n    ## Creating a Vocabulary\n    min_freq = 2\n    unk_token = \"<unk>\"\n    pad_token = \"<pad>\"\n    \n    special_tokens = [\n        unk_token,\n        pad_token,\n        sos_token,\n        eos_token,\n    ]\n    \n    en_vocab = torchtext.vocab.build_vocab_from_iterator(\n        train_data[\"en_tokens\"],\n        min_freq=min_freq,\n        specials=special_tokens,\n    )\n    \n    de_vocab = torchtext.vocab.build_vocab_from_iterator(\n        train_data[\"de_tokens\"],\n        min_freq=min_freq,\n        specials=special_tokens,\n    )\n\n    if rank == 0:\n        print(f\"en vocabulary size: {len(en_vocab)}\")\n        print(f\"de vocabulary size: {len(de_vocab)}\")\n        print(f'Number of training examples: {len(train_data)}')\n        print(f'Number of validation examples: {len(valid_data)}')\n        print(f'Number of testing examples: {len(test_data)}')\n\n\n    assert en_vocab[unk_token] == de_vocab[unk_token]\n    assert en_vocab[pad_token] == de_vocab[pad_token]\n\n    unk_index = en_vocab[unk_token]\n    pad_index = en_vocab[pad_token]\n    en_vocab.set_default_index(unk_index)\n    de_vocab.set_default_index(unk_index)\n\n    ## Numericalizing Data\n    def numericalize_example(example, en_vocab, de_vocab):\n        en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n        de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n        return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n\n    fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n    \n    train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n    valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n    test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n\n\n    data_type = \"torch\"\n    format_columns = [\"en_ids\", \"de_ids\"]\n    \n    train_data = train_data.with_format(\n        type=data_type, columns=format_columns, output_all_columns=True\n    )\n    \n    valid_data = valid_data.with_format(\n        type=data_type,\n        columns=format_columns,\n        output_all_columns=True,\n    )\n    \n    test_data = test_data.with_format(\n        type=data_type,\n        columns=format_columns,\n        output_all_columns=True,\n    )\n\n    ## Creating Data Loaders\n    def get_collate_fn(pad_index):\n        def collate_fn(batch):\n            batch_en_ids = [example[\"en_ids\"] for example in batch]\n            batch_de_ids = [example[\"de_ids\"] for example in batch]\n            batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n            batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n            batch = {\n                \"en_ids\": batch_en_ids,\n                \"de_ids\": batch_de_ids,\n            }\n            return batch\n    \n        return collate_fn\n\n    collate_fn = get_collate_fn(pad_index)\n\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, pin_memory=True) #no sampling for test dataset\n\n\n    \n    return train_dataloader, val_dataloader, test_dataloader, de_vocab, en_vocab, pad_index\n\n\"\"\"# 5. Implement the Training Loop\n\n## a. Help function\n\"\"\"\n\nRESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n\ndef log_results(scenario, results):\n    \"\"\"\n    Save results to a JSON file for comparison across scenarios.\n    \"\"\"\n    if os.path.exists(RESULTS_FILE):\n        with open(RESULTS_FILE, 'r') as f:\n            all_results = json.load(f)\n    else:\n        all_results = {}\n\n    all_results[scenario] = results\n\n    with open(RESULTS_FILE, 'w') as f:\n        json.dump(all_results, f, indent=4)\n\ndef get_accuracy(prediction, label):\n    batch_size, _ = prediction.shape\n    predicted_classes = prediction.argmax(dim=-1)\n    correct_predictions = predicted_classes.eq(label).sum()\n    accuracy = correct_predictions / batch_size\n    return accuracy\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\"\"\"## b. train function\"\"\"\ndef train_fn(\n    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, rank\n):\n    model.train()\n    epoch_loss = 0\n    epoch_accs = 0.0\n    i=0\n    for i, batch in enumerate(data_loader):\n        src = batch[\"de_ids\"].to(rank)\n        trg = batch[\"en_ids\"].to(rank)\n        # src = [src length, batch size]\n        # trg = [trg length, batch size]\n        optimizer.zero_grad()\n        output = model(src, trg, teacher_forcing_ratio)\n        # output = [trg length, batch size, trg vocab size]\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        \n        \n        # output = [(trg length - 1) * batch size, trg vocab size]\n        trg = trg[1:].view(-1)\n        # trg = [(trg length - 1) * batch size]\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        if i % 50 == 0 and rank == 0:\n            print(f\"- On Training: {i} was passed over  {len(data_loader)}\")\n        i+=1\n    return epoch_loss / len(data_loader), epoch_accs\n\n\"\"\"## c. Validation function\"\"\"\ndef evaluate_fn(model, data_loader, criterion, rank,mode = \"Evaluating\"):\n    model.eval()\n    epoch_loss = 0\n    epoch_accs = 0.0\n    i = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            src = batch[\"de_ids\"].to(rank)\n            trg = batch[\"en_ids\"].to(rank)\n            # src = [src length, batch size]\n            # trg = [trg length, batch size]\n            output = model(src, trg, 0)  # turn off teacher forcing\n            # output = [trg length, batch size, trg vocab size]\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            # output = [(trg length - 1) * batch size, trg vocab size]\n            trg = trg[1:].view(-1)\n            # trg = [(trg length - 1) * batch size]\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            if i % 50 == 0 and rank == 0:\n                print(f\"- On {mode}: {i} was passed over  {len(data_loader)}\")\n            i+=1\n    return epoch_loss / len(data_loader), epoch_accs \n\n\"\"\"## d. Main loop\"\"\"\n\noutdir = f'{ROOT}/model/'\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\ndef main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME, clip = CLIP, teacher_forcing_ratio=teacher_forcing_ratio):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    ## b. Create Model, DataLoader\n    train_dataloader, val_dataloader, test_dataloader, de_vocab, en_vocab, pad_index = create_dataloader(rank, world_size)\n    model = create_model(de_vocab, en_vocab).to(rank)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    #LR = 5e-4\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_index).to(rank) # Move loss to GPU\n    optimizer = optim.Adam(model.parameters())\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    train_accurcy = []\n    validation_times = []\n    validation_losses = []\n    validation_accurcy = []\n\n    epoch_times = []\n\n    for epoch in trange(num_epochs, desc=\"Epochs\"):\n        start_epoch_time = time.monotonic()\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train_fn(\n                                                    ddp_model,\n                                                    train_dataloader,\n                                                    optimizer,\n                                                    criterion,\n                                                    clip,\n                                                    teacher_forcing_ratio,\n                                                    rank,\n                                                )\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_accurcy.append(train_acc)\n\n        start_time = time.monotonic()\n        valid_loss, valid_acc = evaluate_fn(\n                                        ddp_model,\n                                        val_dataloader,\n                                        criterion,\n                                        rank,\n                                    )\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_accurcy.append(valid_acc)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}tut-model.pt')\n\n        end_time = time.monotonic()\n        e_time = end_time - start_epoch_time\n        epoch_times.append(e_time)\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss} | Train Acc: {train_acc*100}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100}%')\n\n    ## f. test after train\n    ddp_model.load_state_dict(torch.load(f'{root}tut-model.pt'))\n    start_time = time.monotonic()\n    test_loss, test_acc = evaluate_fn(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n    test_time = time.monotonic() - start_time\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    # Log results\n    results = {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"train_accurcy\": train_accurcy,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n        \"validation_accurcy\": validation_accurcy,\n        \"test_time\": test_time,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"epoch_times\": epoch_times\n     }\n\n    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n    log_results(scenario, results)\n    dist.barrier()\n\n    cleanup()\n    print(f'Process {rank} finished training.')\n\n\"\"\"# 6. Main Execution\"\"\"\nif __name__ == \"__main__\":\n\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size >= 1:\n            #start the training process on all available GPUs\n            if world_size > 1:\n                #start the training process on all available GPUs\n                mp.spawn(\n                    main_train,\n                    args=(world_size,),\n                    nprocs=world_size,\n                    join=True\n                )\n            else:\n                #run training on single GPU\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}