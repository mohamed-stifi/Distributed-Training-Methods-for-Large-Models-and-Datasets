{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Set Up the Environment.","metadata":{"id":"TrZ1iyX7MWSv"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp","metadata":{"id":"1s04bRdcMrMj","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:18:23.199370Z","iopub.execute_input":"2025-01-04T22:18:23.199574Z","iopub.status.idle":"2025-01-04T22:18:28.099655Z","shell.execute_reply.started":"2025-01-04T22:18:23.199554Z","shell.execute_reply":"2025-01-04T22:18:28.098955Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"SEED = 1234\nROOT = '.'\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"id":"8sNi9ly1PQtV","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:18:53.054183Z","iopub.execute_input":"2025-01-04T22:18:53.054505Z","iopub.status.idle":"2025-01-04T22:18:53.064228Z","shell.execute_reply.started":"2025-01-04T22:18:53.054479Z","shell.execute_reply":"2025-01-04T22:18:53.063562Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 2. Initialize the DDP Environment","metadata":{"id":"H08KrVwjMWPu"}},{"cell_type":"code","source":"def setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12355'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()","metadata":{"id":"u58rbHJAMsIT","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:18:58.164950Z","iopub.execute_input":"2025-01-04T22:18:58.165254Z","iopub.status.idle":"2025-01-04T22:18:58.169936Z","shell.execute_reply.started":"2025-01-04T22:18:58.165233Z","shell.execute_reply":"2025-01-04T22:18:58.168809Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 3. Define a Model.","metadata":{"id":"iCZh7WU7Kzqu"}},{"cell_type":"code","source":"class VGG(nn.Module):\n    def __init__(self, features, output_dim):\n        super().__init__()\n\n        self.features = features\n\n        self.avgpool = nn.AdaptiveAvgPool2d(7)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, output_dim),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h","metadata":{"id":"bc5VgWfHMtAO","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:18:59.967475Z","iopub.execute_input":"2025-01-04T22:18:59.967766Z","iopub.status.idle":"2025-01-04T22:18:59.973654Z","shell.execute_reply.started":"2025-01-04T22:18:59.967746Z","shell.execute_reply":"2025-01-04T22:18:59.972788Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_vgg_layers(config, batch_norm):\n\n    layers = []\n    in_channels = 3\n\n    for c in config:\n        assert c == 'M' or isinstance(c, int)\n        if c == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = c\n\n    return nn.Sequential(*layers)","metadata":{"id":"rJV0CnlHRURK","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:04.006803Z","iopub.execute_input":"2025-01-04T22:19:04.007107Z","iopub.status.idle":"2025-01-04T22:19:04.012668Z","shell.execute_reply.started":"2025-01-04T22:19:04.007086Z","shell.execute_reply":"2025-01-04T22:19:04.011695Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\nOUTPUT_DIM = 10\nvgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n\n# model = VGG(vgg11_layers, OUTPUT_DIM)","metadata":{"id":"YaBQRoFLRJ_-","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:04.418214Z","iopub.execute_input":"2025-01-04T22:19:04.418511Z","iopub.status.idle":"2025-01-04T22:19:04.539439Z","shell.execute_reply.started":"2025-01-04T22:19:04.418470Z","shell.execute_reply":"2025-01-04T22:19:04.538696Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"pretrained_model = models.vgg11_bn(pretrained=True)\nIN_FEATURES = pretrained_model.classifier[-1].in_features\n\nfinal_fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n\npretrained_model.classifier[-1] = final_fc\n\n\n# model.load_state_dict(pretrained_model.state_dict())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHIXLuWoS-zm","outputId":"c61dc5ec-fa9b-4b1b-8bc1-b896cd25fd6b","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:05.033277Z","iopub.execute_input":"2025-01-04T22:19:05.033529Z","iopub.status.idle":"2025-01-04T22:19:09.252205Z","shell.execute_reply.started":"2025-01-04T22:19:05.033510Z","shell.execute_reply":"2025-01-04T22:19:09.251513Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n100%|██████████| 507M/507M [00:02<00:00, 211MB/s]  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"id":"dTuEEYy4iw0r","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:13.925002Z","iopub.execute_input":"2025-01-04T22:19:13.925338Z","iopub.status.idle":"2025-01-04T22:19:13.929788Z","shell.execute_reply.started":"2025-01-04T22:19:13.925310Z","shell.execute_reply":"2025-01-04T22:19:13.928621Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def create_model():\n    model = VGG(vgg11_layers, OUTPUT_DIM)\n    print(f'Befor use pretraind: The model has {count_parameters(model):,} trainable parameters')\n\n    model.load_state_dict(pretrained_model.state_dict())\n    for parameter in model.features.parameters():\n        parameter.requires_grad = False\n\n    for parameter in model.classifier[:-1].parameters():\n        parameter.requires_grad = False\n\n    print(f'After use pretraind: The model has {count_parameters(model):,} trainable parameters')\n\n    return model","metadata":{"id":"gEXvywzciUX7","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:14.616645Z","iopub.execute_input":"2025-01-04T22:19:14.616947Z","iopub.status.idle":"2025-01-04T22:19:14.621971Z","shell.execute_reply.started":"2025-01-04T22:19:14.616895Z","shell.execute_reply":"2025-01-04T22:19:14.620975Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 4. Create a Dummy Dataset","metadata":{"id":"CJ0WsdkBK-VJ"}},{"cell_type":"code","source":"def create_dataloader(rank, world_size, batch_size=32):\n    ## defined transforms\n    pretrained_size = 224\n    pretrained_means = [0.485, 0.456, 0.406]\n    pretrained_stds = [0.229, 0.224, 0.225]\n\n    train_transforms = transforms.Compose([\n                              transforms.Resize(pretrained_size),\n                              transforms.RandomRotation(5),\n                              transforms.RandomHorizontalFlip(0.5),\n                              transforms.RandomCrop(pretrained_size, padding=10),\n                              transforms.ToTensor(),\n                              transforms.Normalize(mean=pretrained_means,\n                                                    std=pretrained_stds)\n                          ])\n\n    test_transforms = transforms.Compose([\n                              transforms.Resize(pretrained_size),\n                              transforms.ToTensor(),\n                              transforms.Normalize(mean=pretrained_means,\n                                                    std=pretrained_stds)\n                          ])\n\n    ## load the data with\n    ROOT = f'{ROOT}/data/cifar10'\n\n    train_data = datasets.CIFAR10(ROOT,\n                                  train=True,\n                                  download=True,\n                                  transform=train_transforms)\n\n    test_data = datasets.CIFAR10(ROOT,\n                                train=False,\n                                download=True,\n                                transform=test_transforms)\n\n    ## create the validation split\n    VALID_RATIO = 0.9\n\n    n_train_examples = int(len(train_data) * VALID_RATIO)\n    n_valid_examples = len(train_data) - n_train_examples\n\n    train_data, valid_data = data.random_split(train_data,\n                                              [n_train_examples, n_valid_examples])\n\n\n    ## ensure the validation data uses the test transforms\n    valid_data = copy.deepcopy(valid_data)\n    valid_data.dataset.transform = test_transforms\n\n    ## print\n    print(f'Number of training examples: {len(train_data)}')\n    print(f'Number of validation examples: {len(valid_data)}')\n    print(f'Number of testing examples: {len(test_data)}')\n\n\n\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False) #no sampling for test dataset\n\n\n    return train_dataloader, val_dataloader, test_dataloader","metadata":{"id":"Mgk_bUSBVnqK","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:18.384123Z","iopub.execute_input":"2025-01-04T22:19:18.384441Z","iopub.status.idle":"2025-01-04T22:19:18.391741Z","shell.execute_reply.started":"2025-01-04T22:19:18.384414Z","shell.execute_reply":"2025-01-04T22:19:18.390993Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# 5. Implement the Training Loop","metadata":{"id":"PNOrGaW3LAqT"}},{"cell_type":"markdown","source":"## a. Help function","metadata":{"id":"mDsn5GV6pYFu"}},{"cell_type":"code","source":"def calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"09OQd_QWnqy_","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:26.617391Z","iopub.execute_input":"2025-01-04T22:19:26.617672Z","iopub.status.idle":"2025-01-04T22:19:26.622460Z","shell.execute_reply.started":"2025-01-04T22:19:26.617651Z","shell.execute_reply":"2025-01-04T22:19:26.621636Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## b. train function","metadata":{"id":"iu8QfL2rpuiE"}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, rank):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n\n    for (x, y) in tqdm(iterator, desc= f\"Training on the rank {rank}\", leave=False):\n\n        x = x.to(rank)\n        y = y.to(rank)\n\n        optimizer.zero_grad()\n\n        y_pred, _ = model(x)\n\n        loss = criterion(y_pred, y)\n\n        acc = calculate_accuracy(y_pred, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"id":"tUzHcdwopy1x","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:27.645425Z","iopub.execute_input":"2025-01-04T22:19:27.645743Z","iopub.status.idle":"2025-01-04T22:19:27.650757Z","shell.execute_reply.started":"2025-01-04T22:19:27.645717Z","shell.execute_reply":"2025-01-04T22:19:27.649806Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## c. Validation function","metadata":{"id":"fz0Z96-EpdYV"}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, rank):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.eval()\n\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator, desc=f\"Evaluating on the rank {rank}\", leave=False):\n\n            x = x.to(rank)\n            y = y.to(rank)\n\n            y_pred, _ = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"id":"NwHDFLXTpqxR","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:29.565445Z","iopub.execute_input":"2025-01-04T22:19:29.565728Z","iopub.status.idle":"2025-01-04T22:19:29.571068Z","shell.execute_reply.started":"2025-01-04T22:19:29.565707Z","shell.execute_reply":"2025-01-04T22:19:29.570016Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"outdir = f\"{ROOT}/model/\"\ndef main_train(rank, world_size, root = outdir, num_epochs = 5):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    ## b. Create Model, DataLoader\n    model = create_model().to(rank)  # Move model to GPU\n    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    LR = 5e-4\n    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n    optimizer = optim.Adam(ddp_model.parameters(), lr=LR)\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    for epoch in trange(num_epochs, desc=\"Epochs\"):\n\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}tut4-model.pt')\n\n        end_time = time.monotonic()\n\n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n    ## f. test after train\n    ddp_model.load_state_dict(torch.load(f'{root}tut4-model.pt'))\n    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank)\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    cleanup()\n    print(f'Process {rank} finished training.')","metadata":{"id":"BTerzOhjMviH","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:33.107007Z","iopub.execute_input":"2025-01-04T22:19:33.107284Z","iopub.status.idle":"2025-01-04T22:19:33.114410Z","shell.execute_reply.started":"2025-01-04T22:19:33.107265Z","shell.execute_reply":"2025-01-04T22:19:33.113550Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 6. Main Execution","metadata":{"id":"-dDDhod4LI6I"}},{"cell_type":"code","source":"def main():\n    world_size = torch.cuda.device_count()\n    print(f'Total number of devices detected: {world_size}')\n\n    if world_size > 1:\n        #start the training process on all available GPUs\n        mp.spawn(\n            main_train,\n            args=(world_size, ),\n            nprocs=world_size,\n            join=True\n        )\n\n    else:\n        print('no GPUs found. Please make sure you have configured CUDA correctly')","metadata":{"id":"SglFvktWHevN","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:36.577148Z","iopub.execute_input":"2025-01-04T22:19:36.577434Z","iopub.status.idle":"2025-01-04T22:19:36.581837Z","shell.execute_reply.started":"2025-01-04T22:19:36.577412Z","shell.execute_reply":"2025-01-04T22:19:36.580968Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OD8xZyP8XkjJ","outputId":"fe3a289b-635b-47ca-9d62-f527e42301d2","trusted":true,"execution":{"iopub.status.busy":"2025-01-04T22:19:38.632512Z","iopub.execute_input":"2025-01-04T22:19:38.632787Z","iopub.status.idle":"2025-01-04T22:19:41.131000Z","shell.execute_reply.started":"2025-01-04T22:19:38.632765Z","shell.execute_reply":"2025-01-04T22:19:41.129846Z"}},"outputs":[{"name":"stdout","text":"Total number of devices detected: 2\n","output_type":"stream"},{"name":"stderr","text":"W0104 22:19:41.001000 135128722793600 torch/multiprocessing/spawn.py:146] Terminating process 86 via signal SIGTERM\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-cc6fe0fd3654>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#start the training process on all available GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         mp.spawn(\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmain_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    280\u001b[0m         )\n\u001b[1;32m    281\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 )\n\u001b[1;32m    177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 raise ProcessExitedException(\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0;34m\"process %d terminated with exit code %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"],"ename":"ProcessExitedException","evalue":"process 0 terminated with exit code 1","output_type":"error"}],"execution_count":16},{"cell_type":"markdown","source":"# 7. Examining the Model","metadata":{"id":"hGKlxzUOS84Q"}},{"cell_type":"code","source":"model = VGG(vgg11_layers, OUTPUT_DIM)\nmodel.load_state_dict(torch.load(f'{outdir}tut4-model.pt'))","metadata":{"id":"0dKK57JQWY5C"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predictions(model, iterator):\n\n    model.eval()\n\n    images = []\n    labels = []\n    probs = []\n\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator):\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            y_prob = F.softmax(y_pred, dim=-1)\n\n            images.append(x.cpu())\n            labels.append(y.cpu())\n            probs.append(y_prob.cpu())\n\n    images = torch.cat(images, dim=0)\n    labels = torch.cat(labels, dim=0)\n    probs = torch.cat(probs, dim=0)\n\n    return images, labels, probs\n\ndef plot_confusion_matrix(labels, pred_labels, classes):\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    cm = confusion_matrix(labels, pred_labels)\n    cm = ConfusionMatrixDisplay(cm, display_labels=classes)\n    cm.plot(values_format='d', cmap='Blues', ax=ax)\n    plt.xticks(rotation=20)\n\ndef plot_most_incorrect(incorrect, classes, n_images, normalize=True):\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize=(25, 20))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n\n        image, true_label, probs = incorrect[i]\n        image = image.permute(1, 2, 0)\n        true_prob = probs[true_label]\n        incorrect_prob, incorrect_label = torch.max(probs, dim=0)\n        true_class = classes[true_label]\n        incorrect_class = classes[incorrect_label]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.cpu().numpy())\n        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n'\n                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\n        ax.axis('off')\n\n    fig.subplots_adjust(hspace=0.4)","metadata":{"id":"OsPAvn8iS-wL"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels, probs = get_predictions(model, test_dataloader)\npred_labels = torch.argmax(probs, 1)","metadata":{"id":"ihtNqztMThK3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_confusion_matrix(labels, pred_labels, classes)","metadata":{"id":"CWSm2lVeVw8p"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corrects = torch.eq(labels, pred_labels)\nincorrect_examples = []\n\nfor image, label, prob, correct in zip(images, labels, probs, corrects):\n    if not correct:\n        incorrect_examples.append((image, label, prob))\n\nincorrect_examples.sort(reverse = True,\n                        key=lambda x: torch.max(x[2], dim=0).values)","metadata":{"id":"DoM8E2wpWMqd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_IMAGES = 36\n\nplot_most_incorrect(incorrect_examples, classes, N_IMAGES)","metadata":{"id":"Xn5CmdMUWMmF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\nimport json\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nSEED = 1234\nROOT = \".\"\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\"\"\"# 2. Initialize the DDP Environment\"\"\"\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"# 3. Define a Model.\"\"\"\n\nclass VGG(nn.Module):\n    def __init__(self, features, output_dim):\n        super().__init__()\n\n        self.features = features\n\n        self.avgpool = nn.AdaptiveAvgPool2d(7)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, output_dim),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h\n\ndef get_vgg_layers(config, batch_norm):\n\n    layers = []\n    in_channels = 3\n\n    for c in config:\n        assert c == 'M' or isinstance(c, int)\n        if c == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = c\n\n    return nn.Sequential(*layers)\n\nvgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\nOUTPUT_DIM = 10\nvgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n\n# model = VGG(vgg11_layers, OUTPUT_DIM)\n\npretrained_model = models.vgg11_bn(pretrained=True)\nIN_FEATURES = pretrained_model.classifier[-1].in_features\n\nfinal_fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)\n\npretrained_model.classifier[-1] = final_fc\n\n\n# model.load_state_dict(pretrained_model.state_dict())\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_model():\n    model = VGG(vgg11_layers, OUTPUT_DIM)\n    print(f'Befor use pretraind: The model has {count_parameters(model):,} trainable parameters')\n\n    model.load_state_dict(pretrained_model.state_dict())\n    for parameter in model.features.parameters():\n        parameter.requires_grad = False\n\n    for parameter in model.classifier[:-1].parameters():\n        parameter.requires_grad = False\n\n    print(f'After use pretraind: The model has {count_parameters(model):,} trainable parameters')\n\n    return model\n\n\"\"\"# 4. Create a Dummy Dataset\"\"\"\n\ndef create_dataloader(rank, world_size, batch_size=32, root = ROOT):\n    ## defined transforms\n    pretrained_size = 224\n    pretrained_means = [0.485, 0.456, 0.406]\n    pretrained_stds = [0.229, 0.224, 0.225]\n\n    train_transforms = transforms.Compose([\n                              transforms.Resize(pretrained_size),\n                              transforms.RandomRotation(5),\n                              transforms.RandomHorizontalFlip(0.5),\n                              transforms.RandomCrop(pretrained_size, padding=10),\n                              transforms.ToTensor(),\n                              transforms.Normalize(mean=pretrained_means,\n                                                    std=pretrained_stds)\n                          ])\n\n    test_transforms = transforms.Compose([\n                              transforms.Resize(pretrained_size),\n                              transforms.ToTensor(),\n                              transforms.Normalize(mean=pretrained_means,\n                                                    std=pretrained_stds)\n                          ])\n\n    ## load the data with\n    outdir = f'{root}/data'\n    if rank == 0 and not os.path.exists(outdir):\n        train_data = datasets.CIFAR10(outdir,\n                                  train=True,\n                                  download=True,\n                                  transform=train_transforms)\n\n        test_data = datasets.CIFAR10(outdir,\n                                train=False,\n                                download=True,\n                                transform=test_transforms)\n\n    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n    train_data = datasets.CIFAR10(outdir,\n                                  train=True,\n                                  download=True,\n                                  transform=train_transforms)\n\n    test_data = datasets.CIFAR10(outdir,\n                                train=False,\n                                download=True,\n                                transform=test_transforms)\n    \n    ## create the validation split\n    VALID_RATIO = 0.9\n\n    n_train_examples = int(len(train_data) * VALID_RATIO)\n    n_valid_examples = len(train_data) - n_train_examples\n\n    train_data, valid_data = data.random_split(train_data,\n                                              [n_train_examples, n_valid_examples])\n\n\n    ## ensure the validation data uses the test transforms\n    valid_data = copy.deepcopy(valid_data)\n    valid_data.dataset.transform = test_transforms\n\n    ## print\n    print(f'Number of training examples: {len(train_data)}')\n    print(f'Number of validation examples: {len(valid_data)}')\n    print(f'Number of testing examples: {len(test_data)}')\n\n\n\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n\n\n    return train_dataloader, val_dataloader, test_dataloader\n\n\"\"\"# 5. Implement the Training Loop\n\n## a. Help function\n\"\"\"\nMODEL_NAME = \"VGG11\"\nSENARIO = \"2GPU\"\nEPOCHS = 3\nRESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n\ndef log_results(scenario, results):\n    \"\"\"\n    Save results to a JSON file for comparison across scenarios.\n    \"\"\"\n    if os.path.exists(RESULTS_FILE):\n        with open(RESULTS_FILE, 'r') as f:\n            all_results = json.load(f)\n    else:\n        all_results = {}\n\n    all_results[scenario] = results\n\n    with open(RESULTS_FILE, 'w') as f:\n        json.dump(all_results, f, indent=4)\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\"\"\"## b. train function\"\"\"\n\ndef train(model, iterator, optimizer, criterion, rank):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n    i = 0\n    for (x, y) in tqdm(iterator, desc= f\"Training on the rank {rank}\", leave=False):\n\n        x = x.to(rank)\n        y = y.to(rank)\n\n        optimizer.zero_grad()\n\n        y_pred, _ = model(x)\n\n        loss = criterion(y_pred, y)\n\n        acc = calculate_accuracy(y_pred, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        i+=1\n        if i % 50 == 0 and rank == 0:\n            print(f\"- {i} was passed over {len(iterator)}\")\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n\"\"\"## c. Validation function\"\"\"\n\ndef evaluate(model, iterator, criterion, rank):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.eval()\n    i=0\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator, desc=f\"Evaluating on the rank {rank}\", leave=False):\n\n            x = x.to(rank)\n            y = y.to(rank)\n\n            y_pred, _ = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            i+=1\n            if i % 50 == 0 and rank == 0:\n                print(f\"- {i} was passed over {len(iterator)}\")\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\noutdir = f'{ROOT}/model/'\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\ndef main_train(rank, world_size, root = outdir, num_epochs = 3):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    ## b. Create Model, DataLoader\n    model = create_model().to(rank)  # Move model to GPU\n    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    LR = 5e-4\n    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n    optimizer = optim.Adam(ddp_model.parameters(), lr=LR)\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    train_accurcy = []\n    validation_times = []\n    validation_losses = []\n    validation_accurcy = []\n\n    epoch_times = []\n    \n    for epoch in trange(num_epochs, desc=\"Epochs\"):\n        start_epoch_time = time.monotonic()\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_accurcy.append(train_acc)\n\n        start_time = time.monotonic()\n        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_accurcy.append(valid_acc)\n\n        CHECKPOINT_PATH = f'{root}tut4-model.pt'\n        if rank == 0 and valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n            \n        end_time = time.monotonic()\n        e_time = end_time - start_epoch_time\n        epoch_times.append(e_time)\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n    ## f. test after train\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location, weights_only=True))\n    start_time = time.monotonic()\n    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank)\n    test_time = time.monotonic() - start_time\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    # Log results\n    results = {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"train_accurcy\": train_accurcy,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n        \"validation_accurcy\": validation_accurcy,\n        \"test_time\": test_time,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"epoch_times\": epoch_times\n     }\n\n    model_name = \"vgg11\"\n    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n    log_results(scenario, results)\n    dist.barrier()\n    \n    cleanup()\n    print(f'Process {rank} finished training.')\n\n\"\"\"# 6. Main Execution\"\"\"\nif __name__ == \"__main__\":\n\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size > 1:\n            #start the training process on all available GPUs\n            if world_size > 1:\n                #start the training process on all available GPUs\n                mp.spawn(\n                    main_train,\n                    args=(world_size,),\n                    nprocs=world_size,\n                    join=True\n                )\n            else:\n                #run training on single GPU\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T18:33:23.057846Z","iopub.execute_input":"2025-01-08T18:33:23.058107Z","iopub.status.idle":"2025-01-08T18:33:23.065824Z","shell.execute_reply.started":"2025-01-08T18:33:23.058083Z","shell.execute_reply":"2025-01-08T18:33:23.065004Z"}},"outputs":[{"name":"stdout","text":"Writing main.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T18:33:36.354485Z","iopub.execute_input":"2025-01-08T18:33:36.354771Z","iopub.status.idle":"2025-01-08T18:40:59.339338Z","shell.execute_reply.started":"2025-01-08T18:33:36.354750Z","shell.execute_reply":"2025-01-08T18:40:59.338455Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/hub/checkpoints/vgg11_bn-6002323d.pth\n100%|█████████████████████████████████████████| 507M/507M [00:03<00:00, 171MB/s]\nTotal number of devices detected: 2\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W108 18:33:55.466139490 socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:12345 (errno: 99 - Cannot assign requested address).\nProcess 0 initialized.\nProcess 1 initialized.\nBefor use pretraind: The model has 128,812,810 trainable parameters\nBefor use pretraind: The model has 128,812,810 trainable parameters\nAfter use pretraind: The model has 40,970 trainable parameters\nAfter use pretraind: The model has 40,970 trainable parameters\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:02<00:00, 68877144.52it/s]\nExtracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nEpochs:   0%|          | 0/3 [00:00<?, ?it/s]\nEpochs:   0%|          | 0/3 [00:00<?, ?it/s]\nTraining on the rank 1:   0%|          | 0/704 [00:00<?, ?it/s]\nTraining on the rank 0:   0%|          | 0/704 [00:00<?, ?it/s]\n- 50 was passed over 704\n- 100 was passed over 704\n- 150 was passed over 704\n- 200 was passed over 704\n- 250 was passed over 704\n- 300 was passed over 704\n- 350 was passed over 704\n- 400 was passed over 704\n- 450 was passed over 704\n- 500 was passed over 704\n- 550 was passed over 704\n- 600 was passed over 704\n- 650 was passed over 704\n- 700 was passed over 704\nEvaluating on the rank 0:   0%|          | 0/79 [00:00<?, ?it/s]\nEvaluating on the rank 1:   0%|          | 0/79 [00:00<?, ?it/s]\n- 50 was passed over 79\n--------------|     On process 1      |----------------\nEpoch: 01 | Epoch Time: 2m 0s\n\tTrain Loss: 0.914 | Train Acc: 68.92%\n\t Val. Loss: 0.673 |  Val. Acc: 76.66%\nTraining on the rank 1:   0%|          | 0/704 [00:00<?, ?it/s]\n--------------|     On process 0      |----------------\nEpoch: 01 | Epoch Time: 2m 1s\n\tTrain Loss: 0.908 | Train Acc: 69.03%\n\t Val. Loss: 0.679 |  Val. Acc: 76.62%\nTraining on the rank 0:   0%|          | 0/704 [00:00<?, ?it/s]\n- 50 was passed over 704\n- 100 was passed over 704\n- 150 was passed over 704\n- 200 was passed over 704\n- 250 was passed over 704\n- 300 was passed over 704\n- 350 was passed over 704\n- 400 was passed over 704\n- 450 was passed over 704\n- 500 was passed over 704\n- 550 was passed over 704\n- 600 was passed over 704\n- 650 was passed over 704\n- 700 was passed over 704\nEvaluating on the rank 1:   0%|          | 0/79 [00:00<?, ?it/s]\nEvaluating on the rank 0:   0%|          | 0/79 [00:00<?, ?it/s]\n- 50 was passed over 79\n--------------|     On process 1      |----------------\nEpoch: 02 | Epoch Time: 2m 3s\n\tTrain Loss: 0.781 | Train Acc: 72.84%\n\t Val. Loss: 0.626 |  Val. Acc: 78.32%\nTraining on the rank 1:   0%|          | 0/704 [00:00<?, ?it/s]\n--------------|     On process 0      |----------------\nEpoch: 02 | Epoch Time: 2m 4s\n\tTrain Loss: 0.782 | Train Acc: 72.76%\n\t Val. Loss: 0.627 |  Val. Acc: 78.64%\nTraining on the rank 0:   0%|          | 0/704 [00:00<?, ?it/s]\n- 50 was passed over 704\n- 100 was passed over 704\n- 150 was passed over 704\n- 200 was passed over 704\n- 250 was passed over 704\n- 300 was passed over 704\n- 350 was passed over 704\n- 400 was passed over 704\n- 450 was passed over 704\n- 500 was passed over 704\n- 550 was passed over 704\n- 600 was passed over 704\n- 650 was passed over 704\n- 700 was passed over 704\nEvaluating on the rank 0:   0%|          | 0/79 [00:00<?, ?it/s]\nEvaluating on the rank 1:   0%|          | 0/79 [00:00<?, ?it/s]\n- 50 was passed over 79\n--------------|     On process 0      |----------------\nEpoch: 03 | Epoch Time: 2m 3s\n\tTrain Loss: 0.770 | Train Acc: 73.01%\n\t Val. Loss: 0.630 |  Val. Acc: 77.73%\n--------------|     On process 1      |----------------\nEpoch: 03 | Epoch Time: 2m 4s\n\tTrain Loss: 0.773 | Train Acc: 72.78%\n\t Val. Loss: 0.616 |  Val. Acc: 78.64%\nEvaluating on the rank 0:   0%|          | 0/313 [00:00<?, ?it/s]\nEvaluating on the rank 1:   0%|          | 0/313 [00:00<?, ?it/s]\n- 50 was passed over 313\n- 100 was passed over 313\n- 150 was passed over 313\n- 200 was passed over 313\n- 250 was passed over 313\n- 300 was passed over 313\nTest results on process 0: Test Loss: 0.636 | Test Acc: 77.49%\nTest results on process 1: Test Loss: 0.636 | Test Acc: 77.49%\nProcess 0 finished training.\nProcess 1 finished training.\n","output_type":"stream"}],"execution_count":2}]}