{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T19:25:56.118713Z","iopub.execute_input":"2025-01-09T19:25:56.118953Z","iopub.status.idle":"2025-01-09T19:26:00.823351Z","shell.execute_reply.started":"2025-01-09T19:25:56.118931Z","shell.execute_reply":"2025-01-09T19:26:00.822478Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torchtext==0.17.0 torch==2.2.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T19:27:14.241421Z","iopub.execute_input":"2025-01-09T19:27:14.241779Z","iopub.status.idle":"2025-01-09T19:29:35.345842Z","shell.execute_reply.started":"2025-01-09T19:27:14.241755Z","shell.execute_reply":"2025-01-09T19:29:35.345009Z"}},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchdata, torchtext\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.1+cu121\n    Uninstalling torch-2.4.1+cu121:\n      Successfully uninstalled torch-2.4.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.2.0 which is incompatible.\ntorchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile main.py\n\nimport datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchtext\n\nimport tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\nimport json\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nSEED = 1234\nROOT = \".\"\nMODEL_NAME = \"NBoW\"\nSENARIO = \"2GPU\"\nEPOCHS = 20\nBATCH_SIZE = 512\n\noutdir = \"./my_datasets\"  \nos.makedirs(outdir, exist_ok=True)\nos.environ['HF_DATASETS_CACHE'] = outdir\n\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\"\"\"# 2. Initialize the DDP Environment\"\"\"\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"# 3. Define a Model.\"\"\"\n\nclass NBoW(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n        self.fc = nn.Linear(embedding_dim, output_dim)\n\n    def forward(self, ids):\n        # ids = [batch size, seq len]\n        embedded = self.embedding(ids)\n        # embedded = [batch size, seq len, embedding dim]\n        pooled = embedded.mean(dim=1)\n        # pooled = [batch size, embedding dim]\n        prediction = self.fc(pooled)\n        # prediction = [batch size, output dim]\n        return prediction\n\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_model(vocab, output_dim, pad_index, embedding_dim = 300):\n    vocab_size = len(vocab)\n    model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)\n    print(f'The model has {count_parameters(model):,} trainable parameters')\n\n    vectors = torchtext.vocab.GloVe()\n    pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n\n    model.embedding.weight.data = pretrained_embedding\n\n    return model\n\n\"\"\"# 4. Create a Dummy Dataset\"\"\"\n\ndef create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n    def tokenize_example(example, tokenizer, max_length):\n        tokens = tokenizer(example[\"text\"])[:max_length]\n        return {\"tokens\": tokens}\n\n    ## load the data with\n    if rank == 0:\n        # Load the dataset \n        train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n\n    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n     \n    train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n    \n    ## Tokenization\n    tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n    train_data = train_data.map(\n        tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n    )\n    test_data = test_data.map(\n        tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n    )\n\n    ## create the validation split\n    test_size = 0.25\n\n    train_valid_data = test_data.train_test_split(test_size=test_size)\n    test_data = train_valid_data[\"train\"]\n    valid_data = train_valid_data[\"test\"]\n\n\n    ## Creating a Vocabulary\n    min_freq = 5\n    special_tokens = [\"<unk>\", \"<pad>\"]\n\n    vocab = torchtext.vocab.build_vocab_from_iterator(\n        train_data[\"tokens\"],\n        min_freq=min_freq,\n        specials=special_tokens,\n    )\n\n    if rank == 0:\n        print(f\"Vocabulary size: {len(vocab)}\")\n        print(f'Number of training examples: {len(train_data)}')\n        print(f'Number of validation examples: {len(valid_data)}')\n        print(f'Number of testing examples: {len(test_data)}')\n\n    unk_index = vocab[\"<unk>\"]\n    pad_index = vocab[\"<pad>\"]\n    vocab.set_default_index(unk_index)\n\n    ## Numericalizing Data\n    def numericalize_example(example, vocab):\n        ids = vocab.lookup_indices(example[\"tokens\"])\n        return {\"ids\": ids}\n\n    train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n    valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n    test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n\n    train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n    valid_data = valid_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n    test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n\n    ## Creating Data Loaders\n    def get_collate_fn(pad_index):\n        def collate_fn(batch):\n            batch_ids = [i[\"ids\"] for i in batch]\n            batch_ids = nn.utils.rnn.pad_sequence(\n                batch_ids, padding_value=pad_index, batch_first=True\n            )\n            batch_label = [i[\"label\"] for i in batch]\n            batch_label = torch.stack(batch_label)\n            batch = {\"ids\": batch_ids, \"label\": batch_label}\n            return batch\n\n        return collate_fn\n    \n    collate_fn = get_collate_fn(pad_index)\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, pin_memory=True) #no sampling for test dataset\n\n    \n    output_dim = len(train_data.unique(\"label\"))\n    return train_dataloader, val_dataloader, test_dataloader, vocab, output_dim, pad_index\n\n\"\"\"# 5. Implement the Training Loop\n\n## a. Help function\n\"\"\"\n\nRESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n\ndef log_results(scenario, results):\n    \"\"\"\n    Save results to a JSON file for comparison across scenarios.\n    \"\"\"\n    if os.path.exists(RESULTS_FILE):\n        with open(RESULTS_FILE, 'r') as f:\n            all_results = json.load(f)\n    else:\n        all_results = {}\n\n    all_results[scenario] = results\n\n    with open(RESULTS_FILE, 'w') as f:\n        json.dump(all_results, f, indent=4)\n\ndef get_accuracy(prediction, label):\n    batch_size, _ = prediction.shape\n    predicted_classes = prediction.argmax(dim=-1)\n    correct_predictions = predicted_classes.eq(label).sum()\n    accuracy = correct_predictions / batch_size\n    return accuracy\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\"\"\"## b. train function\"\"\"\ndef train(model, data_loader, criterion, optimizer, rank):\n    model.train()\n    epoch_losses = []\n    epoch_accs = []\n    i=0\n    for batch in tqdm.tqdm(data_loader, desc=f\"Training on the rank {rank}...\"):\n        ids = batch[\"ids\"].to(rank)\n        label = batch[\"label\"].to(rank)\n        prediction = model(ids)\n        loss = criterion(prediction, label)\n        accuracy = get_accuracy(prediction, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        epoch_losses.append(loss.item())\n        epoch_accs.append(accuracy.item())\n        if i % 50 == 0 and rank == 0:\n            print(f\"- On Training: {i} was passed over  {len(data_loader)}\")\n        i+=1\n    return np.mean(epoch_losses), np.mean(epoch_accs)\n\n\"\"\"## c. Validation function\"\"\"\n\ndef evaluate(model, data_loader, criterion, rank, mode = \"Evaluating\"):\n    model.eval()\n    epoch_losses = []\n    epoch_accs = []\n    i = 0\n    with torch.no_grad():\n        for batch in tqdm.tqdm(data_loader, desc=f\"{mode} on the rank {rank}...\"):\n            ids = batch[\"ids\"].to(rank)\n            label = batch[\"label\"].to(rank)\n            prediction = model(ids)\n            loss = criterion(prediction, label)\n            accuracy = get_accuracy(prediction, label)\n            epoch_losses.append(loss.item())\n            epoch_accs.append(accuracy.item())\n            if i % 50 == 0 and rank == 0:\n                print(f\"- On {mode}: {i} was passed over  {len(data_loader)}\")\n            i+=1\n    return np.mean(epoch_losses), np.mean(epoch_accs)\n\n\"\"\"## d. Main loop\"\"\"\n\noutdir = f'{ROOT}/model/'\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\ndef main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    ## b. Create Model, DataLoader\n    train_dataloader, val_dataloader, test_dataloader, vocab, output_dim, pad_index = create_dataloader(rank, world_size)\n    model = create_model(vocab, output_dim, pad_index).to(rank)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    #LR = 5e-4\n    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n    optimizer = optim.Adam(ddp_model.parameters())\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    train_accurcy = []\n    validation_times = []\n    validation_losses = []\n    validation_accurcy = []\n\n    epoch_times = []\n    \n    for epoch in range(num_epochs):\n        start_epoch_time = time.monotonic()\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train(ddp_model, train_dataloader, criterion, optimizer, rank)\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_accurcy.append(train_acc)\n\n        start_time = time.monotonic()\n        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_accurcy.append(valid_acc)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}tut-model.pt')\n\n        end_time = time.monotonic()\n        e_time = end_time - start_epoch_time\n        epoch_times.append(e_time)\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n    ## f. test after train\n    ddp_model.load_state_dict(torch.load(f'{root}tut-model.pt'))\n    start_time = time.monotonic()\n    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n    test_time = time.monotonic() - start_time\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    # Log results\n    results = {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"train_accurcy\": train_accurcy,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n        \"validation_accurcy\": validation_accurcy,\n        \"test_time\": test_time,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"epoch_times\": epoch_times\n     }\n\n    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n    log_results(scenario, results)\n    dist.barrier()\n    \n    cleanup()\n    print(f'Process {rank} finished training.')\n\n\"\"\"# 6. Main Execution\"\"\"\nif __name__ == \"__main__\":\n\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size >= 1:\n            #start the training process on all available GPUs\n            if world_size > 1:\n                #start the training process on all available GPUs\n                mp.spawn(\n                    main_train,\n                    args=(world_size,),\n                    nprocs=world_size,\n                    join=True\n                )\n            else:\n                #run training on single GPU\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T22:31:19.286792Z","iopub.execute_input":"2025-01-08T22:31:19.287147Z","iopub.status.idle":"2025-01-08T22:31:19.294685Z","shell.execute_reply.started":"2025-01-08T22:31:19.287118Z","shell.execute_reply":"2025-01-08T22:31:19.293768Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T22:31:25.733515Z","iopub.execute_input":"2025-01-08T22:31:25.733866Z","iopub.status.idle":"2025-01-08T22:31:25.851931Z","shell.execute_reply.started":"2025-01-08T22:31:25.733835Z","shell.execute_reply":"2025-01-08T22:31:25.851133Z"}},"outputs":[{"name":"stdout","text":"main.py  \u001b[0m\u001b[01;34mmodel\u001b[0m/  \u001b[01;34mmy_datasets\u001b[0m/  NBoW_5epochs_2GPU.json\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T22:31:26.175201Z","iopub.execute_input":"2025-01-08T22:31:26.175451Z"}},"outputs":[{"name":"stdout","text":"Total number of devices detected: 2\nProcess 0 initialized.\nProcess 1 initialized.\nVocabulary size: 24897\nNumber of training examples: 25000\nNumber of validation examples: 6250\nNumber of testing examples: 18750\nThe model has 7,469,702 trainable parameters\nThe model has 7,469,702 trainable parameters\nTraining on the rank 0...:   0%|                         | 0/25 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}