{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile main.py\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport spacy\nimport datasets\nimport torchtext\nimport tqdm\n# import evaluate\n\n\n\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\nimport json\n\nimport torch.utils.data as data\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nSEED = 1234\nROOT = \".\"\nMODEL_NAME = \"seq2seq\"\nSENARIO = \"2GPU\"\nEPOCHS = 10\nCLIP = 1.0\nteacher_forcing_ratio = 0.5\nBATCH_SIZE = 512\n\noutdir = \"./my_datasets\"\nos.makedirs(outdir, exist_ok=True)\nos.environ['HF_DATASETS_CACHE'] = outdir\n\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\"\"\"# 2. Initialize the DDP Environment\"\"\"\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"# 3. Define a Model.\"\"\"\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        # src = [src length, batch size]\n        embedded = self.dropout(self.embedding(src))\n        # embedded = [src length, batch size, embedding dim]\n        outputs, (hidden, cell) = self.rnn(embedded)\n        # outputs = [src length, batch size, hidden dim * n directions]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # cell = [n layers * n directions, batch size, hidden dim]\n        # outputs are always from the top hidden layer\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell):\n        # input = [batch size]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # cell = [n layers * n directions, batch size, hidden dim]\n        # n directions in the decoder will both always be 1, therefore:\n        # hidden = [n layers, batch size, hidden dim]\n        # context = [n layers, batch size, hidden dim]\n        input = input.unsqueeze(0)\n        # input = [1, batch size]\n        embedded = self.dropout(self.embedding(input))\n        # embedded = [1, batch size, embedding dim]\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        # output = [seq length, batch size, hidden dim * n directions]\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # cell = [n layers * n directions, batch size, hidden dim]\n        # seq length and n directions will always be 1 in this decoder, therefore:\n        # output = [1, batch size, hidden dim]\n        # hidden = [n layers, batch size, hidden dim]\n        # cell = [n layers, batch size, hidden dim]\n        prediction = self.fc_out(output.squeeze(0))\n        # prediction = [batch size, output dim]\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        assert (\n            encoder.hidden_dim == decoder.hidden_dim\n        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert (\n            encoder.n_layers == decoder.n_layers\n        ), \"Encoder and decoder must have equal number of layers!\"\n\n    def forward(self, src, trg, teacher_forcing_ratio):\n        # src = [src length, batch size]\n        # trg = [trg length, batch size]\n        # teacher_forcing_ratio is probability to use teacher forcing\n        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        batch_size = trg.shape[1]\n        trg_length = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        # tensor to store decoder outputs\n        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(trg.device)\n        # last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        # hidden = [n layers * n directions, batch size, hidden dim]\n        # cell = [n layers * n directions, batch size, hidden dim]\n        # first input to the decoder is the <sos> tokens\n        input = trg[0, :]\n        # input = [batch size]\n        for t in range(1, trg_length):\n            # insert input token embedding, previous hidden and previous cell states\n            # receive output tensor (predictions) and new hidden and cell states\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            # output = [batch size, output dim]\n            # hidden = [n layers, batch size, hidden dim]\n            # cell = [n layers, batch size, hidden dim]\n            # place predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            # decide if we are going to use teacher forcing or not\n            teacher_force = random.random() < teacher_forcing_ratio\n            # get the highest predicted token from our predictions\n            top1 = output.argmax(1)\n            # if teacher forcing, use actual next token as next input\n            # if not, use predicted token\n            input = trg[t] if teacher_force else top1\n            # input = [batch size]\n        return outputs\n\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n\ndef create_model(de_vocab, en_vocab):\n    input_dim = len(de_vocab)\n    output_dim = len(en_vocab)\n    encoder_embedding_dim = 256\n    decoder_embedding_dim = 256\n    hidden_dim = 512\n    n_layers = 2\n    encoder_dropout = 0.5\n    decoder_dropout = 0.5\n\n    encoder = Encoder(\n        input_dim,\n        encoder_embedding_dim,\n        hidden_dim,\n        n_layers,\n        encoder_dropout,\n    )\n\n    decoder = Decoder(\n        output_dim,\n        decoder_embedding_dim,\n        hidden_dim,\n        n_layers,\n        decoder_dropout,\n    )\n\n    model = Seq2Seq(encoder, decoder)\n    print(f'The model has {count_parameters(model):,} trainable parameters')\n    model.apply(init_weights)\n    return model\n\n\"\"\"# 4. Create a Dummy Dataset\"\"\"\n\ndef create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n    def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n        en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n        de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n        if lower:\n            en_tokens = [token.lower() for token in en_tokens]\n            de_tokens = [token.lower() for token in de_tokens]\n        en_tokens = [sos_token] + en_tokens + [eos_token]\n        de_tokens = [sos_token] + de_tokens + [eos_token]\n        return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n    ## load the data with\n    if rank == 0:\n        dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n\n    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n\n    dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n\n    train_data, valid_data, test_data = (\n        dataset[\"train\"],\n        dataset[\"validation\"],\n        dataset[\"test\"],\n    )\n\n    ## Tokenization\n    en_nlp = spacy.load(\"en_core_web_sm\")\n    de_nlp = spacy.load(\"de_core_news_sm\")\n\n    max_length = 1_000\n    lower = True\n    sos_token = \"<sos>\"\n    eos_token = \"<eos>\"\n\n    fn_kwargs = {\n        \"en_nlp\": en_nlp,\n        \"de_nlp\": de_nlp,\n        \"max_length\": max_length,\n        \"lower\": lower,\n        \"sos_token\": sos_token,\n        \"eos_token\": eos_token,\n    }\n\n    train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n    valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n    test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n\n    ## create the validation split\n\n\n    ## Creating a Vocabulary\n    min_freq = 2\n    unk_token = \"<unk>\"\n    pad_token = \"<pad>\"\n\n    special_tokens = [\n        unk_token,\n        pad_token,\n        sos_token,\n        eos_token,\n    ]\n\n    en_vocab = torchtext.vocab.build_vocab_from_iterator(\n        train_data[\"en_tokens\"],\n        min_freq=min_freq,\n        specials=special_tokens,\n    )\n\n    de_vocab = torchtext.vocab.build_vocab_from_iterator(\n        train_data[\"de_tokens\"],\n        min_freq=min_freq,\n        specials=special_tokens,\n    )\n\n    if rank == 0:\n        print(f\"en vocabulary size: {len(en_vocab)}\")\n        print(f\"de vocabulary size: {len(de_vocab)}\")\n        print(f'Number of training examples: {len(train_data)}')\n        print(f'Number of validation examples: {len(valid_data)}')\n        print(f'Number of testing examples: {len(test_data)}')\n\n\n    assert en_vocab[unk_token] == de_vocab[unk_token]\n    assert en_vocab[pad_token] == de_vocab[pad_token]\n\n    unk_index = en_vocab[unk_token]\n    pad_index = en_vocab[pad_token]\n    en_vocab.set_default_index(unk_index)\n    de_vocab.set_default_index(unk_index)\n\n    ## Numericalizing Data\n    def numericalize_example(example, en_vocab, de_vocab):\n        en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n        de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n        return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n\n    fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n\n    train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n    valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n    test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n\n\n    data_type = \"torch\"\n    format_columns = [\"en_ids\", \"de_ids\"]\n\n    train_data = train_data.with_format(\n        type=data_type, columns=format_columns, output_all_columns=True\n    )\n\n    valid_data = valid_data.with_format(\n        type=data_type,\n        columns=format_columns,\n        output_all_columns=True,\n    )\n\n    test_data = test_data.with_format(\n        type=data_type,\n        columns=format_columns,\n        output_all_columns=True,\n    )\n\n    ## Creating Data Loaders\n    def get_collate_fn(pad_index):\n        def collate_fn(batch):\n            batch_en_ids = [example[\"en_ids\"] for example in batch]\n            batch_de_ids = [example[\"de_ids\"] for example in batch]\n            batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n            batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n            batch = {\n                \"en_ids\": batch_en_ids,\n                \"de_ids\": batch_de_ids,\n            }\n            return batch\n\n        return collate_fn\n\n    collate_fn = get_collate_fn(pad_index)\n\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, pin_memory=True) #no sampling for test dataset\n\n\n    \n    return train_dataloader, val_dataloader, test_dataloader, de_vocab, en_vocab, pad_index\n\n\"\"\"# 5. Implement the Training Loop\n\n## a. Help function\n\"\"\"\n\nRESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n\ndef log_results(scenario, results):\n    \"\"\"\n    Save results to a JSON file for comparison across scenarios.\n    \"\"\"\n    if os.path.exists(RESULTS_FILE):\n        with open(RESULTS_FILE, 'r') as f:\n            all_results = json.load(f)\n    else:\n        all_results = {}\n\n    all_results[scenario] = results\n\n    with open(RESULTS_FILE, 'w') as f:\n        json.dump(all_results, f, indent=4)\n\ndef get_accuracy(prediction, label):\n    batch_size, _ = prediction.shape\n    predicted_classes = prediction.argmax(dim=-1)\n    correct_predictions = predicted_classes.eq(label).sum()\n    accuracy = correct_predictions / batch_size\n    return accuracy\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\"\"\"## b. train function\"\"\"\ndef train_fn(\n    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, rank\n):\n    model.train()\n    epoch_loss = 0\n    epoch_accs = 0.0\n    i=0\n    for i, batch in enumerate(data_loader):\n        src = batch[\"de_ids\"].to(rank)\n        trg = batch[\"en_ids\"].to(rank)\n        # src = [src length, batch size]\n        # trg = [trg length, batch size]\n        optimizer.zero_grad()\n        output = model(src, trg, teacher_forcing_ratio)\n        # output = [trg length, batch size, trg vocab size]\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        \n        \n        # output = [(trg length - 1) * batch size, trg vocab size]\n        trg = trg[1:].view(-1)\n        # trg = [(trg length - 1) * batch size]\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        if i % 50 == 0 and rank == 0:\n            print(f\"- On Training: {i} was passed over  {len(data_loader)}\")\n        i+=1\n    return epoch_loss / len(data_loader), epoch_accs\n\n\"\"\"## c. Validation function\"\"\"\ndef evaluate_fn(model, data_loader, criterion, rank,mode = \"Evaluating\"):\n    model.eval()\n    epoch_loss = 0\n    epoch_accs = 0.0\n    i = 0\n    with torch.no_grad():\n        for i, batch in enumerate(data_loader):\n            src = batch[\"de_ids\"].to(rank)\n            trg = batch[\"en_ids\"].to(rank)\n            # src = [src length, batch size]\n            # trg = [trg length, batch size]\n            output = model(src, trg, 0)  # turn off teacher forcing\n            # output = [trg length, batch size, trg vocab size]\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            # output = [(trg length - 1) * batch size, trg vocab size]\n            trg = trg[1:].view(-1)\n            # trg = [(trg length - 1) * batch size]\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            if i % 50 == 0 and rank == 0:\n                print(f\"- On {mode}: {i} was passed over  {len(data_loader)}\")\n            i+=1\n    return epoch_loss / len(data_loader), epoch_accs \n\n\"\"\"## d. Main loop\"\"\"\n\noutdir = f'{ROOT}/model/'\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\ndef main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME, clip = CLIP, teacher_forcing_ratio=teacher_forcing_ratio):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    ## b. Create Model, DataLoader\n    train_dataloader, val_dataloader, test_dataloader, de_vocab, en_vocab, pad_index = create_dataloader(rank, world_size)\n    model = create_model(de_vocab, en_vocab).to(rank)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    #LR = 5e-4\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_index).to(rank) # Move loss to GPU\n    optimizer = optim.Adam(model.parameters())\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    train_accurcy = []\n    validation_times = []\n    validation_losses = []\n    validation_accurcy = []\n\n    epoch_times = []\n\n    for epoch in trange(num_epochs, desc=\"Epochs\"):\n        start_epoch_time = time.monotonic()\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train_fn(\n                                                    ddp_model,\n                                                    train_dataloader,\n                                                    optimizer,\n                                                    criterion,\n                                                    clip,\n                                                    teacher_forcing_ratio,\n                                                    rank,\n                                                )\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_accurcy.append(train_acc)\n\n        start_time = time.monotonic()\n        valid_loss, valid_acc = evaluate_fn(\n                                        ddp_model,\n                                        val_dataloader,\n                                        criterion,\n                                        rank,\n                                    )\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_accurcy.append(valid_acc)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}tut-model.pt')\n\n        end_time = time.monotonic()\n        e_time = end_time - start_epoch_time\n        epoch_times.append(e_time)\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss} | Train Acc: {train_acc*100}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100}%')\n\n    ## f. test after train\n    ddp_model.load_state_dict(torch.load(f'{root}tut-model.pt'))\n    start_time = time.monotonic()\n    test_loss, test_acc = evaluate_fn(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n    test_time = time.monotonic() - start_time\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    # Log results\n    results = {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"train_accurcy\": train_accurcy,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n        \"validation_accurcy\": validation_accurcy,\n        \"test_time\": test_time,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"epoch_times\": epoch_times\n     }\n\n    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n    log_results(scenario, results)\n    dist.barrier()\n\n    cleanup()\n    print(f'Process {rank} finished training.')\n\n\"\"\"# 6. Main Execution\"\"\"\nif __name__ == \"__main__\":\n\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size >= 1:\n            #start the training process on all available GPUs\n            if world_size > 1:\n                #start the training process on all available GPUs\n                mp.spawn(\n                    main_train,\n                    args=(world_size,),\n                    nprocs=world_size,\n                    join=True\n                )\n            else:\n                #run training on single GPU\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T17:24:29.240888Z","iopub.execute_input":"2025-01-09T17:24:29.241200Z","iopub.status.idle":"2025-01-09T17:24:29.249687Z","shell.execute_reply.started":"2025-01-09T17:24:29.241169Z","shell.execute_reply":"2025-01-09T17:24:29.248869Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T17:24:31.537238Z","iopub.execute_input":"2025-01-09T17:24:31.537550Z","iopub.status.idle":"2025-01-09T17:29:26.968270Z","shell.execute_reply.started":"2025-01-09T17:24:31.537520Z","shell.execute_reply":"2025-01-09T17:29:26.967240Z"}},"outputs":[{"name":"stdout","text":"Total number of devices detected: 2\nProcess 1 initialized.\nProcess 0 initialized.\nen vocabulary size: 5893\nde vocabulary size: 7853\nNumber of training examples: 29000\nNumber of validation examples: 1014\nNumber of testing examples: 1000\nThe model has 13,898,501 trainable parameters\nThe model has 13,898,501 trainable parameters\nEpochs:   0%|          | 0/10 [00:00<?, ?it/s]\nEpochs:   0%|          | 0/10 [00:00<?, ?it/s]\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 01 | Epoch Time: 0m 26s\n\tTrain Loss: 5.783839110670419 | Train Acc: 0.0%\n\t Val. Loss: 5.031 |  Val. Acc: 0.0%\n--------------|     On process 1      |----------------\nEpoch: 01 | Epoch Time: 0m 26s\n\tTrain Loss: 5.797069927741742 | Train Acc: 0.0%\n\t Val. Loss: 5.004 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 1      |----------------\n--------------|     On process 0      |----------------\nEpoch: 02 | Epoch Time: 0m 27s\nEpoch: 02 | Epoch Time: 0m 27s\n\tTrain Loss: 5.030860802222943 | Train Acc: 0.0%\n\tTrain Loss: 5.03876204326235 | Train Acc: 0.0%\n\t Val. Loss: 4.957 |  Val. Acc: 0.0%\n\t Val. Loss: 4.927 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n--------------|     On process 1      |----------------\nEpoch: 03 | Epoch Time: 0m 27s\n\tTrain Loss: 4.964020827720905 | Train Acc: 0.0%\n\t Val. Loss: 4.948 |  Val. Acc: 0.0%\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 03 | Epoch Time: 0m 27s\n\tTrain Loss: 4.958552015238795 | Train Acc: 0.0%\n\t Val. Loss: 4.986 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n--------------|     On process 1      |----------------\nEpoch: 04 | Epoch Time: 0m 27s\n\tTrain Loss: 4.894055152761525 | Train Acc: 0.0%\n\t Val. Loss: 4.929 |  Val. Acc: 0.0%\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 04 | Epoch Time: 0m 27s\n\tTrain Loss: 4.881615556519607 | Train Acc: 0.0%\n\t Val. Loss: 4.958 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\n--------------|     On process 1      |----------------\nEpoch: 05 | Epoch Time: 0m 27s\nEpoch: 05 | Epoch Time: 0m 27s\n\tTrain Loss: 4.753777553295267 | Train Acc: 0.0%\n\tTrain Loss: 4.744670029344229 | Train Acc: 0.0%\n\t Val. Loss: 4.904 |  Val. Acc: 0.0%\n\t Val. Loss: 4.953 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n--------------|     On process 1      |----------------\nEpoch: 06 | Epoch Time: 0m 27s\n\tTrain Loss: 4.604174531739334 | Train Acc: 0.0%\n\t Val. Loss: 4.945 |  Val. Acc: 0.0%\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 06 | Epoch Time: 0m 27s\n\tTrain Loss: 4.581569556532235 | Train Acc: 0.0%\n\t Val. Loss: 4.993 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 1      |----------------\n--------------|     On process 0      |----------------\nEpoch: 07 | Epoch Time: 0m 27s\nEpoch: 07 | Epoch Time: 0m 27s\n\tTrain Loss: 4.478304188826988 | Train Acc: 0.0%\n\tTrain Loss: 4.4820307534316495 | Train Acc: 0.0%\n\t Val. Loss: 4.838 |  Val. Acc: 0.0%\n\t Val. Loss: 4.865 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 08 | Epoch Time: 0m 27s\n\tTrain Loss: 4.361374591958934 | Train Acc: 0.0%\n\t Val. Loss: 4.747 |  Val. Acc: 0.0%\n--------------|     On process 1      |----------------\nEpoch: 08 | Epoch Time: 0m 27s\n\tTrain Loss: 4.37106283779802 | Train Acc: 0.0%\n\t Val. Loss: 4.731 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n--------------|     On process 1      |----------------\nEpoch: 09 | Epoch Time: 0m 27s\n\tTrain Loss: 4.282366801952493 | Train Acc: 0.0%\n\t Val. Loss: 4.827 |  Val. Acc: 0.0%\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\nEpoch: 09 | Epoch Time: 0m 27s\n\tTrain Loss: 4.274001918990036 | Train Acc: 0.0%\n\t Val. Loss: 4.850 |  Val. Acc: 0.0%\n- On Training: 0 was passed over  29\n- On Evaluating: 0 was passed over  1\n--------------|     On process 0      |----------------\n--------------|     On process 1      |----------------\nEpoch: 10 | Epoch Time: 0m 27s\nEpoch: 10 | Epoch Time: 0m 27s\n\tTrain Loss: 4.224428119330571 | Train Acc: 0.0%\n\tTrain Loss: 4.196270942687988 | Train Acc: 0.0%\n\t Val. Loss: 4.689 |  Val. Acc: 0.0%\n\t Val. Loss: 4.677 |  Val. Acc: 0.0%\n- On Testing: 0 was passed over  2\nTest results on process 1: Test Loss: 4.658 | Test Acc: 0.00%\nTest results on process 0: Test Loss: 4.658 | Test Acc: 0.00%\nProcess 1 finished training.\nProcess 0 finished training.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install torchtext==0.17.0 torch==2.2.0\n!pip install datasets\n!python -m spacy download en_core_web_sm\n!python -m spacy download de_core_news_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T16:56:49.078876Z","iopub.execute_input":"2025-01-09T16:56:49.079180Z","iopub.status.idle":"2025-01-09T16:59:40.520722Z","shell.execute_reply.started":"2025-01-09T16:56:49.079153Z","shell.execute_reply":"2025-01-09T16:59:40.519656Z"}},"outputs":[{"name":"stdout","text":"Collecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nDownloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchdata, torchtext\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.1+cu121\n    Uninstalling torch-2.4.1+cu121:\n      Successfully uninstalled torch-2.4.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.2.0 which is incompatible.\ntorchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 triton-2.2.0\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting de-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\nInstalling collected packages: de-core-news-sm\nSuccessfully installed de-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}